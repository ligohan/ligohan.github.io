[{"title":"【Elasticsearch】13.性能测试（esrally 安装和使用）","date":"2018-05-11T08:54:40.752Z","path":"2018/05/11/Elasticsearch/13.性能测试（esrally安装和使用）/","text":"如何进行压测 自己写代码，难以确保测试代码的专业性 esperf 和 elasticsearch-stress-test http 压测工具。es 对外暴露了 Restful API，因此所有的针对 http 协议的压测工具都可以用来测试 es，比如 JMeter、httpload等等。 elastic 官方工具 esrally esrally 介绍 esrally 是 elastic 官方开源的一款基于 python3 实现的针对 es 的压测工具，源码地址。 esrally主要功能如下： 自动创建、压测和销毁 es 集群 可分 es 版本管理压测数据和方案 完善的压测数据展示，支持不同压测之间的数据对比分析，也可以将数据存储到指定的es中进行二次分析 支持收集 JVM 详细信息，比如内存、GC等数据来定位性能问题 官方文档 esrally 安装 环境要求 Python 3.4+ 和 pip3 JDK 8 git 1.9+ 安装 Python 和 pip3 centOS7 安装 Python3.5 安装pip3 12yum install python34-setuptoolseasy_install-3.4 pip 之后就可以使用pip3了,如： install numpy```1234- **安装 git** 1. 安装编译git时需要的包 yum install curl-devel expat-devel gettext-devel openssl-devel zlib-develyum install gcc perl-ExtUtils-MakeMaker 12. 删除已有的git yum remove git 13. 下载git源码 cd /opt/setups ## 随意进入哪个目录，用来下载git使用wget https://www.kernel.org/pub/software/scm/git/git-2.9.1.tar.gztar xzf git-2.9.1.tar.gz 14. 编译安装 cd git-2.9.1./configure –prefix=/usr/program/git_2.9.1 ## 安装git的目录makemake install 15. 创建软连接 ln -s /usr/program/git_2.9.1/bin/* /usr/bin/ ## 如果安装在/usr/local下系统环境变量无需要做软链接 16. 测试git git –version 123- **安装esrally** pip3 install esrally1- 安装过程若报错```unable to execute &apos;gcc&apos;: No such file or directory``` ，执行如下命令 $ yum list | grep gcc$ sudo yum install -y gcc.x86_6412- 首次配置 esrally configure 命令后将会检测你的环境，以及需要填写JAVA_HOME。查看JAVA_HOME路径[root@jun ~] ls -l /etc/alternatives/javalrwxrwxrwx. 1 root root 72 4月 9 23:29 /etc/alternatives/java -&gt; /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre/bin/java 就是它了 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre12- 启动esrally esrally list tracks123456789101112131415![image](https://i.imgur.com/beHc3F8.png)## 相关术语- rally：汽车拉力赛，esrally 是将压测比作了汽车拉力赛，因此其中的很多术语都是从汽车拉力赛中借鉴来的- track：赛道，指压测用的数据和测试策略，[详细文档](http://esrally.readthedocs.io/en/latest/track.html)，esrally 自带的track都在 [github](https://github.com/elastic/rally-tracks) 上 ，有很多测试数据，比如 geonames geopoint logging nested 等，每个数据文件夹中的 README.md 中有详细的数据介绍，而 track.json 便是压测策略的定义文件。&gt; - track.json 文件说明&gt; - data-url: 一个url地址，指明测试数据的下载根路径，与下方 indices 中的 documents 结合，可得到数据的下载地址。&gt; - indices: 指定该track可以操作的索引，包括创建、更新、删除等操作。详细信息可以参见这里。&gt; - operations: 指定具体的操作，比如 index 索引数据的操作、force-merge 强制合并segment的操作、search 搜索的操作等等。具体例子可以看下面的示例。详细信息可以参见这里。&gt; - challenges: 通过组合 operations 定义一系列 task ，再组合成一个压测的流程，请参照下方的 例子。详细信息可以参见[这里](http://esrally.readthedocs.io/en/latest/track.html#challenges)。&gt; challenges/default.json 中的一个定义如下：&gt; { “name”: “append-no-conflicts”, “description”: “”, “default”: true, “index-settings”: { “index.number_of_replicas”: 0 }, “schedule”: [ { “operation”: “index-append”, “warmup-time-period”: 240, “clients”: 8 }, { “operation”: “force-merge”, “clients”: 1 }, ···}123456&gt; 这里定义了一个名为 append-no-conflicts 的 challenge。&gt; 由于每次压测只能运行一个challenge，这里的 default 参数是指当压测未指定时默认运行的 challenge。&gt; schedule 中指定了该 challenge 中按顺序执行 index-append、force-merge、index-stats 等 9 个task，除此之外还可以设定 clients （并发客户端数）、warmup-iterations（预热的循环次数）、iterations（operation 执行的循环次数）等，详情请参见此处。&gt; &gt; 通过下面的命令可以查看当前 esrally 可以使用的track。&gt; esrally list tracks123- car：赛车，指不同配置的 es 实例&gt;通过下面的命令可以查看 esrally 当前可用的 car。&gt; esrally list cars1234&gt; cars 配置位于 rally 目录(mac默认是 ~/.rally)中 benchmarks/teams/default/cars/ 下面。具体配置可以参见 [cars 的文档](http://esrally.readthedocs.io/en/latest/car.html)，除了 heap 的配置，所有的 es 配置都可以修改。- race：比赛，指某一次压测。如果不指定赛车，就用 default 配置，如果不指定赛道，则默认使用 geonames track。&gt; 通过下面的命令来执行一次 race。&gt; esrally race –track=logging –challenge=append-no-conflicts –car=”4gheap”1234&gt; 指定使用 logging 的track，运行该 track 中的 append-no-conflicts 的 challenge，指定的 car 为 4gheap 的 es 实例，详情见 [race 的文档](http://esrally.readthedocs.io/en/latest/race.html)- Tournament：锦标赛，由多个 race 组成&gt; 通过下面的命令可以查看所有的 race。&gt; esrally list races1&gt; Recent races: Race Timestamp Track Challenge Car User Tag 20160518T122341Z pmc append-no-conflicts defaults intention:reduce_alloc_123420160518T112057Z pmc append-no-conflicts defaults intention:baseline_github_123420160518T101957Z pmc append-no-conflicts defaults12&gt; 当有了多个 race 后，可以通过下面的命令方便地比较不同 race 之间的数据。&gt; esrally compare –baseline=20160518T112057Z –contender=20160518T112341Z123- Pipeline：压测的一个流程&gt; 通过下面的命令可以查看已有的pipeline。&gt; esrally list pipeline1&gt; Name Description from-sources-complete Builds and provisions Elasticsearch, runs a benchmark and reports results.from-sources-skip-build Provisions Elasticsearch (skips the build), runs a benchmark and reports results.from-distribution Downloads an Elasticsearch distribution, provisions it, runs a benchmark and reports results.benchmark-only Assumes an already running Elasticsearch instance, runs a benchmark and reports results123456789101112131415161718192021222324&gt; - from-sources-complete：是从源代码编译 es 后再运行，可以通过 --revision 参数指明要编译的commit hash ，这样就可以针对某一个提交版本就行测试了。&gt; - from-sources-skip-build：如果已经编译好了，使用该 pipeline，可以跳过编译的流程，节省测试时间&gt; - from-distribution：通过 --distribution-version 指定 es 版本，esrally 会从官网直接下载该版本的可执行文件，然后进行测试。&gt; - **benchmark-only**：将 es 集群的管理交由用户来处理， esrally 只做压测。如果你想针对已有集群进行测试，那么要将pipeline设定为该模式。&gt; &gt; 详细信息见 [pipeline 的文档](http://esrally.readthedocs.io/en/latest/pipelines.html)## 压测流程1. 根据参数设定自行编译或者下载 es 可执行实例，然后根据 car 的约定，创建并启动 es 集群。如果使用 benchmark-only 的pipeline，则该步骤省略。2. 根据指定 track 去下载数据，然后按照指定的 challenge 进行操作。3. 记录并输出压测结果数据。## 压测结果分析- 压测结束后，esrally 会将结果输出到终端和结果文件（位于 esrally 目录logs 和 benchmarks/races）中![image](https://segmentfault.com/img/remote/1460000011174702)- 在 Metric 一栏，有非常多的指标数据，详细的解释可以参见[该文档]()。一般要关注的数据有： - throughput：每个操作的吞吐量，比如 index、search等 - latency：每个操作的响应时长数据 - Heap used for x：记录堆栈的使用情况- 每一次压测都会以压测时的时间命名，比如 logs/rally_out_20170822T082858Z.log - 在 benchmarks/races/2017-08-22-08-28-58 中记录着最终的结果和 es 的运行日志- 对于 benchmark-only 模式的测试，即针对已有集群的压力测试，也**可以通过安装 X-Pack Basic 版本进行监控（Monitoring）**，在压测的过程中就能查看相关指标。- esrally 可以在配置的时候指定将所有的 race 压测结果数据存入一个指定的 es 实例中，配置如下(在 esrally 目录中 rally.ini 文件中)： vim /root/.rally/rally.ini 修改以下数据[reporting]datastore.type = elasticsearchdatastore.host = localhostdatastore.port = 9200datastore.secure = Falsedatastore.user =datastore.password =12345678910111213- esrally 会将数据存储在如下 3 个index中，下面 * 代指月份，即按月存储结果数据。 - rally-metrics-* 该索引分指标记录每次 race 的结果，如下图所示为某一次race的所有 metric 数据。![image](https://segmentfault.com/img/remote/1460000011174705)- 第一列时间是指某一次压测的时间- 第二列时间是指标采集的时间- 第三列 operation 指具体执行的操作- operation 为空的指标都是总计类的，比如indexing total time 记录的是总索引数据的时间、segments_count 是总段数等等- 其他的 operation 都记录了每一个操作的数据。需要注意的是，这里记录的是 operation 的所有采样数据，不是一个最终的汇总数据。上面截图中也可以看出同一个 hour_agg 的operation 有多项名为 service_time 的指标数据，但他们的采集时间是不同的。## 实战### 测试已有集群的性能 esrally race –pipeline=benchmark-only –target-hosts=127.0.0.1:9200 –cluster-health=yellow –track=nyc_taxis –challenge=append-no-conflicts –cluster-health=yellow 默认 esrally 会检查集群状态，非 green 状态会直接退出。添加该参数可以避免该情况` 参考资料Elasticsearch 压测方案之 esrally 简介","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"},{"name":"esrally","slug":"esrally","permalink":"http://blog.shaib.cn/tags/esrally/"}]},{"title":"【Elasticsearch】12.集群监控","date":"2018-05-11T08:54:40.748Z","path":"2018/05/11/Elasticsearch/12.集群监控/","text":"集群健康1GET _cluster/health 返回数据 1234567891011121314151617&#123; &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;status&quot;: &quot;green&quot;, &quot;timed_out&quot;: false, &quot;number_of_nodes&quot;: 2, ## 集群节点数 &quot;number_of_data_nodes&quot;: 2, ## 数据节点数量 &quot;active_primary_shards&quot;: 31, ## 主分片数量 &quot;active_shards&quot;: 57, ## 可用的分片数量 &quot;relocating_shards&quot;: 0, ## 正在重新分配的分片数量，在新加或者减少节点的时候会发生 &quot;initializing_shards&quot;: 0, ## 正在初始化的分片数量，新建索引或者刚启动会存在，时间很短 &quot;unassigned_shards&quot;: 0, ## 没有分配的分片，一般就是那些名存实不存的副本分片 &quot;delayed_unassigned_shards&quot;: 0, &quot;number_of_pending_tasks&quot;: 0, &quot;number_of_in_flight_fetch&quot;: 0, &quot;task_max_waiting_in_queue_millis&quot;: 0, &quot;active_shards_percent_as_number&quot;: 100&#125; status：集群状态 Status 含义 green 所有的主分片和副本分片都已分配。你的集群是 100% 可用的。 yellow 有的主分片已经分片了，但至少还有一个副本是缺失的。不会有数据丢失，所以搜索结果依然是完整的。 red 至少一个主分片以及它的全部副本都在缺失中。搜索只能返回部分数据，而分配到这个分片上的写入请求会返回一个异常。 索引级别集群状态，可以细致查看到底是哪个索引引起集群的故障的 1GET _cluster/health?level=indices 分片级别集群状态，可以细致查看到底是哪个分片引起的集群故障 1GET _cluster/health?level=shards 阻塞查看集群状态，适用于自动化脚本。当状态变为指定状态或者更好就返回继续执行。 1GET _cluster/health?wait_for_status=yellow 集群统计信息 索引级别的统计信息，比节点级别的统计信息详细。但是并不很实用 123GET my_index/_stats ## 统计 my_index 索引GET my_index,another_index/_stats ## 使用逗号分隔索引名可以请求多个索引统计值GET _all/_stats ## 请求全部索引的统计值 节点级别状态信息，各部分节点数据的解释请参看网址 1GET _nodes/stats 集群所有统计信息 1curl _cluster/stats 较为有用的指标： nodes.successful nodes.failed nodes.total nodes.mem.used_percent nodes.process.cpu.percent nodes.jvm.mem.heap_used","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"}]},{"title":"【Elasticsearch】11.线上部署优化","date":"2018-05-11T08:54:40.743Z","path":"2018/05/11/Elasticsearch/11.线上部署优化/","text":"配置JVM HEAP MAP 修改 /config/jvm.options12-Xms4g-Xmx4g 最大内存不要超过32G- [官方说明](https://www.elastic.co/guide/cn/elasticsearch/guide/current/heap-sizing.html) 禁止内存交互 防止elasticsearch进程的内存被交换到磁盘，引起性能的急剧下降 将config/elasticsearch.yml中的bootstrap.mlockall项设为true 1bootstrap.memory_lock true 设置好之后，用下面的命令检查返回的mlockall设置项是否为true 1curl http://localhost:9200/_nodes/process?pretty 修改系统允许的最大文件打开数 使用下面命令然后查看open file的数字1$ ulimit -a 将最大文件打开数调整为204801234567$ sudo vim /etc/security/limits.conf&quot;&quot;* soft nofile 20480* hard nofile 20480&quot;&quot;$ sudo reboot -h now 清除缓存1curl -XPOST &quot;localhost:9200/_cache/clear&quot;","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"}]},{"title":"【Elasticsearch】10.多表 Join (6.x 新类型)","date":"2018-05-11T08:54:40.739Z","path":"2018/05/11/Elasticsearch/10.多表Join（6.x新类型）/","text":"产生背景 Mysql中多表关联，我们可以通过 left join 或者 Join 等实现； ES5.X版本，借助父子文档实现多表关联，类似数据库中 Join 的功能；实现的核心是借助于ES5.X支持1个索引(index)下多个类型(type)。 ES6.X版本，每个索引下面只支持单一的类型（type）。 Join 类型介绍 仍然是一个索引下，借助父子关系，实现类似Mysql中多表关联的操作。 使用 Mapping定义 123456789101112131415PUT my_join_index&#123; &quot;mappings&quot;: &#123; &quot;_doc&quot;: &#123; &quot;properties&quot;: &#123; &quot;my_join_field&quot;: &#123; ## join 的名称 &quot;type&quot;: &quot;join&quot;, &quot;relations&quot;: &#123; &quot;question&quot;: &quot;answer&quot; ## 意为 qustion 为 answer 的父类 &#125; &#125; &#125; &#125; &#125;&#125; 定义父文档 1234567891011PUT my_join_index/_doc/1?refresh&#123; &quot;text&quot;: &quot;This is a question&quot;, &quot;my_join_field&quot;: &quot;question&quot; &#125;PUT my_join_index/_doc/2?refresh&#123; &quot;text&quot;: &quot;This is another question&quot;, &quot;my_join_field&quot;: &quot;question&quot;&#125; 定义子文档 1234567891011121314151617PUT my_join_index/_doc/3?routing=1&amp;refresh ## 路由值是强制性的，因为父文件和子文件必须在相同的分片上建立索引&#123; &quot;text&quot;: &quot;This is an answer&quot;, &quot;my_join_field&quot;: &#123; &quot;name&quot;: &quot;answer&quot;, ## &quot;answer&quot;是此子文档的加入名称 &quot;parent&quot;: &quot;1&quot; ## 指定此子文档的父文档ID：1 &#125;&#125;PUT my_join_index/_doc/4?routing=1&amp;refresh&#123; &quot;text&quot;: &quot;This is another answer&quot;, &quot;my_join_field&quot;: &#123; &quot;name&quot;: &quot;answer&quot;, &quot;parent&quot;: &quot;1&quot; &#125;&#125; 约束 每个索引只允许一个Join类型Mapping定义； 父文档和子文档必须在同一个分片上编入索引；这意味着，当进行删除、更新、查找子文档时候需要提供相同的路由值。 一个文档可以有多个子文档，但只能有一个父文档。 可以为已经存在的Join类型添加新的关系。 当一个文档已经成为父文档后，可以为该文档添加子文档。 检索与聚合基于父文档查找子文档parent_id 根据父文档ID查询123456789GET my_join_index/_search&#123; &quot;query&quot;: &#123; &quot;parent_id&quot;: &#123; &quot;type&quot;: &quot;answer&quot;, &quot;id&quot;: &quot;1&quot; &#125; &#125;&#125; has_parent查询12345678910111213GET my_join_index/_search&#123; &quot;query&quot;: &#123; &quot;has_parent&quot; : &#123; &quot;parent_type&quot; : &quot;question&quot;, &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;text&quot; : &quot;This is&quot; &#125; &#125; &#125; &#125;&#125; 返回数据1234567891011121314151617181920212223242526272829303132333435363738394041424344&#123; &quot;took&quot;: 90, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_join_index&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_routing&quot;: &quot;1&quot;, &quot;_source&quot;: &#123; &quot;text&quot;: &quot;This is an answer&quot;, &quot;my_join_field&quot;: &#123; &quot;name&quot;: &quot;answer&quot;, &quot;parent&quot;: &quot;1&quot; &#125; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_join_index&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1, &quot;_routing&quot;: &quot;1&quot;, &quot;_source&quot;: &#123; &quot;text&quot;: &quot;This is another answer&quot;, &quot;my_join_field&quot;: &#123; &quot;name&quot;: &quot;answer&quot;, &quot;parent&quot;: &quot;1&quot; &#125; &#125; &#125; ] &#125;&#125; 基于子文档查找父文档has_child 查询12345678910111213GET my_join_index/_search&#123; &quot;query&quot;: &#123; &quot;has_child&quot; : &#123; &quot;type&quot; : &quot;answer&quot;, &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;text&quot; : &quot;This is question&quot; &#125; &#125; &#125; &#125;&#125; 返回数据1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 24, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_join_index&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;text&quot;: &quot;This is a question&quot;, &quot;my_join_field&quot;: &quot;question&quot; &#125; &#125; ] &#125;&#125; 聚合123456789101112131415161718192021222324GET my_join_index/_search&#123; &quot;query&quot;: &#123; &quot;parent_id&quot;: &#123; ## parent_id是特定的检索方式，用于检索属于特定父文档id=1的，子文档类型为answer的文档的个数 &quot;type&quot;: &quot;answer&quot;, &quot;id&quot;: &quot;1&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;parents&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;my_join_field#question&quot;, ## 基于父文档类型question进行聚合 &quot;size&quot;: 10 &#125; &#125; &#125;, &quot;script_fields&quot;: &#123; &quot;parent&quot;: &#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;doc[&apos;my_join_field#question&apos;]&quot; ## 在搜索结果添加一个 parent 字段，值为 my_join_field#question &#125; &#125; &#125;&#125; 一对多一对多定义123456789101112131415PUT join_ext_index&#123; &quot;mappings&quot;: &#123; &quot;_doc&quot;: &#123; &quot;properties&quot;: &#123; &quot;my_join_field&quot;: &#123; &quot;type&quot;: &quot;join&quot;, &quot;relations&quot;: &#123; &quot;question&quot;: [&quot;answer&quot;, &quot;comment&quot;] &#125; &#125; &#125; &#125; &#125;&#125; 一对多对多定义12345678910111213141516171819202122232425PUT join_multi_index&#123; &quot;mappings&quot;: &#123; &quot;_doc&quot;: &#123; &quot;properties&quot;: &#123; &quot;my_join_field&quot;: &#123; &quot;type&quot;: &quot;join&quot;, &quot;relations&quot;: &#123; &quot;question&quot;: [&quot;answer&quot;, &quot;comment&quot;], &quot;answer&quot;: &quot;vote&quot; &#125; &#125; &#125; &#125; &#125;&#125;## 图解 question / \\ / \\comment answer | | vote 孙子文档导入数据.12345678PUT join_multi_index/_doc/3?routing=1&amp;refresh &#123; &quot;text&quot;: &quot;This is a vote&quot;, &quot;my_join_field&quot;: &#123; &quot;name&quot;: &quot;vote&quot;, &quot;parent&quot;: &quot;2&quot; &#125;&#125; 参考资料 Elasticsearch 6.X 新类型Join深入详解","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"}]},{"title":"【Elasticsearch】9.深度分页","date":"2018-05-11T08:54:40.734Z","path":"2018/05/11/Elasticsearch/9.深度分页/","text":"什么是深分页问题？ 如果想查询第5000-5100条数据，发送如下查询条件就可以做到： 12345POST auditlog_operation/operlog/_search&#123; “from”:5000 //from：定义从哪里开始拿数据 “size”:100 //size：定义一共拿多少条数据&#125; 查询流程如下： 客户端发送请求到某个node节点。 此node将请求广播到各分片，各分片各自查询前5100条数据。 查询结果返回给node节点，node对结果进行合并整合，取出前5100条数据。 返回给客户端。 这样的查询在10000-50000条数据（1000到5000页）以内的时候还是可以的 但是如果数据过多的话，就会出现深分页问题： 如果你要深度获取1000000到1000100页的数据，性能问题会非常明显的暴露出来：CPU、内存、IO、网络带宽等等，而且Elasticsearch本身就是个Java应用，若并发上去，Elasticsearch会快就会 OOM 解决方法scroll 方式 scroll 方式原理就是通过每次查询后，返回一个scroll_id。根据这个scroll_id 进行下一页的查询 可以把这个scroll_id理解为通常关系型数据库中的游标。但是，这种scroll方式的缺点是不能够进行反复查询，也就是说，只能进行下一页，不能进行上一页 假设测试索引共有10条文档，每次取出4条 首先取出前4条，并且得到scroll_id 123456789101112131415161718192021POST /cars/transactions/_search?scroll=3m ## 这里的3m代表的是持续滚动时间，如果过了3分钟，还没有查询下一页，那么这个scroll_id就会失效&#123; &quot;size&quot;: 3&#125;----返回数据----&#123; &quot;_scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAACZCFkVhdzd1Skd4UkotRTBCSVc5NTRtS2cAAAAAAAA8JhZJY1dNOHBZMlFEU3VBT0VpZmZGenBBAAAAAAAAPCgWSWNXTThwWTJRRFN1QU9FaWZmRnpwQQAAAAAAACZDFkVhdzd1Skd4UkotRTBCSVc5NTRtS2cAAAAAAAA8JxZJY1dNOHBZMlFEU3VBT0VpZmZGenBB&quot;, &quot;took&quot;: 6, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; ··· &#125;&#125; 再次查询下一页，注意，这里查询时不需要指定index，只需要指定scroll_id和本次的持续滚动时间。 1234567POST /_search/scroll&#123; &quot;scroll&quot; : &quot;3m&quot;, &quot;scroll_id&quot;:&quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAACZCFkVhdzd1Skd4UkotRTBCSVc5NTRtS2cAAAAAAAA8JhZJY1dNOHBZMlFEU3VBT0VpZmZGenBBAAAAAAAAPCgWSWNXTThwWTJRRFN1QU9FaWZmRnpwQQAAAAAAACZDFkVhdzd1Skd4UkotRTBCSVc5NTRtS2cAAAAAAAA8JxZJY1dNOHBZMlFEU3VBT0VpZmZGenBB&quot;&#125;----再次返回3条数据---- 再次根据返回的 scroll_id 查询 发现这次只有两条数据，我们再次根据返回的scroll_id查询 已经没有数据了，说明已经滚动到最后了 这个时候可以删除这个scroll_id 使用如下方法： 1DELETE /_search/scroll/DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAueFjZoLTd3UEptUXFXR2Z6Nm5iZ3FqalEAAAAAAAALnBY2aC03d1BKbVFxV0dmejZuYmdxampRAAAAAAAAC58WNmgtN3dQSm1RcVdHZno2bmJncWpqUQAAAAAAAAudFjZoLTd3UEptUXFXR2Z6Nm5iZ3FqalEAAAAAAAALoBY2aC03d1BKbVFxV0dmejZuYmdxampR 也可以删除所有scroll_id： /_search/scroll/_all```1234567### Search After- Elasticsearch5.x 新特性- [官方文档](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-search-after.html)- 首次查询语句 GET cars/_search{ “size”: 2, “query”: { “match_all” : { } }, &quot;sort&quot;: [ {&quot;_id&quot;: &quot;desc&quot;}, {&quot;price&quot;: &quot;desc&quot;} ] }12- 需要使用一个每个文档的值都是独有的字段作为排序条件，建议使用 _id- 返回数据 { ··· “hits”: [ { “_index”: “cars”, “_type”: “transactions”, “_id”: “CG8kzWIBWwL7t5InLKFv”, “_score”: null, “_source”: { “price”: 25000, “color”: “blue”, “make”: “ford”, “sold”: “2014-02-12” }, “sort”: [ “CG8kzWIBWwL7t5InLKFv”, 25000 ] }, { “_index”: “cars”, “_type”: “transactions”, “_id”: “Bm8kzWIBWwL7t5InLKFv”, “_score”: null, “_source”: { “price”: 20000, “color”: “red”, “make”: “honda”, “sold”: “2014-11-05” }, “sort”: [ “Bm8kzWIBWwL7t5InLKFv”, 20000 ] } ] }}12- 可以看到每一个返回的文档都有一个 sort 数组，**将它作为下一次查询的search_after参数值**- 继续查询，使用 search_after 参数 GET cars/_search{ “size”: 2, “query”: { “match_all” : { } }, &quot;search_after&quot;: [&quot;Bm8kzWIBWwL7t5InLKFv&quot;, 20000], ## 上一个请求返回的值 &quot;sort&quot;: [ {&quot;_id&quot;: &quot;desc&quot;}, {&quot;price&quot;: &quot;desc&quot;} ] }12- 使用 search_after 时 from 参数一定要设置成 0 或 -1- 返回数据 { ··· “hits”: { “total”: 8, “max_score”: null, “hits”: [ { “_index”: “cars”, “_type”: “transactions”, “_id”: “BW8kzWIBWwL7t5InLKFv”, “_score”: null, “_source”: { “price”: 12000, “color”: “green”, “make”: “toyota”, “sold”: “2014-08-19” }, “sort”: [ “BW8kzWIBWwL7t5InLKFv”, 12000 ] }, { “_index”: “cars”, “_type”: “transactions”, “_id”: “BG8kzWIBWwL7t5InLKFv”, “_score”: null, “_source”: { “price”: 15000, “color”: “blue”, “make”: “toyota”, “sold”: “2014-07-02” }, “sort”: [ “BG8kzWIBWwL7t5InLKFv”, 15000 ] } ] }}`","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"}]},{"title":"【Elasticsearch】8.聚合","date":"2018-05-11T08:54:40.729Z","path":"2018/05/11/Elasticsearch/8.聚合/","text":"聚合 在Elasticsearch的聚合中需要掌握两个核心的概念：桶（bucket）、指标（metric） 桶（bucket）: 满足特定条件的文档的集 简单来说桶就是满足特定条件的文档的集合。 当聚合开始被执行，每个文档里面的值通过计算来决定符合哪个桶的条件，如果匹配到，文档将放入相应的桶并接着开始聚合操作。 桶也可以被嵌套在其他桶里面。 指标（metric）: 对桶内的文档进行聚合分析的操作 桶能让我们划分文档到有意义的集合，但是最终我们需要的是对这些桶内的文档进行一些指标的计算。分桶是一种达到目的地的手段：它提供了一种给文档分组的方法来让我们可以计算感兴趣的指标 大多数指标是简单的数学运算（如：最小值、平均值、最大值、汇总），这些是通过文档的值来计算的 每个聚合都是一个或者多个桶和零个或者多个指标的组合，聚合可能只有一个桶，可能只有一个指标，或者可能两个都有。也有可能一些桶嵌套在其他桶里面 翻译成粗略的SQL语句来解释的话：123SELECT COUNT(color) ---&gt; 相当于指标FROM tableGROUP BY color ---&gt; 相当于桶 简单聚合 先批量索引一些数据： 1234567891011121314151617POST /cars/transactions/_bulk&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 10000, &quot;color&quot; : &quot;red&quot;, &quot;make&quot; : &quot;honda&quot;, &quot;sold&quot; : &quot;2014-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 20000, &quot;color&quot; : &quot;red&quot;, &quot;make&quot; : &quot;honda&quot;, &quot;sold&quot; : &quot;2014-11-05&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 30000, &quot;color&quot; : &quot;green&quot;, &quot;make&quot; : &quot;ford&quot;, &quot;sold&quot; : &quot;2014-05-18&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 15000, &quot;color&quot; : &quot;blue&quot;, &quot;make&quot; : &quot;toyota&quot;, &quot;sold&quot; : &quot;2014-07-02&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 12000, &quot;color&quot; : &quot;green&quot;, &quot;make&quot; : &quot;toyota&quot;, &quot;sold&quot; : &quot;2014-08-19&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 20000, &quot;color&quot; : &quot;red&quot;, &quot;make&quot; : &quot;honda&quot;, &quot;sold&quot; : &quot;2014-11-05&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 80000, &quot;color&quot; : &quot;red&quot;, &quot;make&quot; : &quot;bmw&quot;, &quot;sold&quot; : &quot;2014-01-01&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 25000, &quot;color&quot; : &quot;blue&quot;, &quot;make&quot; : &quot;ford&quot;, &quot;sold&quot; : &quot;2014-02-12&quot; &#125; 汽车经销商可能会想知道哪个颜色的汽车销量最好，用聚合可以轻易得到结果，用 terms 桶操作： 1234567891011GET /cars/transactions/_search&#123; &quot;size&quot; : 0, ## 不显示 hits 原数据，只显示聚合统计结果，提高查询速度 &quot;aggs&quot; : &#123; ## 完整形式 aggregations 同样有效 &quot;popular_colors&quot; : &#123; ## 可以为聚合指定一个我们想要名称 &quot;terms&quot; : &#123; ## 定义单个桶的类型 &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125;&#125; 此时应该会报出异常： “Fielddata is disabled on text fields by default. Set fielddata=true on [color] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.” 出现该错误是因为5.x之后，Elasticsearch 对排序、聚合所依据的字段用单独的数据结构(fielddata)缓存到内存里了，但是在text字段上默认是禁用的，如果有需要单独开启，这样做的目的是为了节省内存空间。——官方文档地址 解决方法：将color字段的fielddata设置为true 123456789PUT /cars/_mapping/transactions&#123; &quot;properties&quot;: &#123; &quot;color&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: true &#125; &#125;&#125; 重试聚合操作，返回结果： 1234567891011121314151617181920212223242526272829303132333435&#123; &quot;took&quot;: 48, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;popular_colors&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;red&quot;, &quot;doc_count&quot;: 4 &#125;, &#123; &quot;key&quot;: &quot;blue&quot;, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: &quot;green&quot;, &quot;doc_count&quot;: 2 &#125; ] &#125; &#125;&#125; 每个桶的 key 都与 color 字段里找到的唯一词对应。它总会包含 doc_count 字段，告诉我们包含该词项的文档数量 度量 将度量 嵌套 在桶内， 度量会基于桶内的文档计算统计结果 例如为汽车的例子加入 average 平均度量： 123456789101112131415161718GET /cars/transactions/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; ## 为度量新增 aggs 层 &quot;avg_price&quot;: &#123; ## 为度量指定名字： avg_price &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; ## 为 price 字段定义 avg 度量 &#125; &#125; &#125; &#125; &#125;&#125; 搜索由 toyota 制作的每个颜色的汽车销量和均价，并按均价倒序输出 1234567891011121314151617181920212223242526GET /cars/transactions/_search&#123; &quot;size&quot; : 0, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;make&quot;: &quot;toyota&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot;, &quot;order&quot;: &#123; &quot;avg_price&quot;: &quot;desc&quot; ## 根据均价倒序输出 &#125; &#125;, &quot;aggs&quot;: &#123; ## 为度量新增 aggs 层 &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; ## 为 price 字段定义 avg 度量 &#125; &#125; &#125; &#125; &#125;&#125; 嵌套桶 查询每个颜色的汽车制造商的分布 1234567891011121314151617181920212223GET /cars/transactions/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;make&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;make&quot; &#125; &#125; &#125; &#125; &#125;&#125; 返回数据 123456789101112131415161718192021222324252627&#123;... &quot;aggregations&quot;: &#123; &quot;colors&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;red&quot;, &quot;doc_count&quot;: 4, ## 红色车有四辆 &quot;make&quot;: &#123; ## 在原有基础上，为每一个桶嵌入了一个新的聚合 &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;honda&quot;, ## 红色车中的三辆是 Honda 本田制造，一辆是 BMW 宝马制造 &quot;doc_count&quot;: 3 &#125;, &#123; &quot;key&quot;: &quot;bmw&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 32500 ## 红色车的平均售价是 $32，500 美元 &#125; &#125;,...&#125; 条形图histogram 柱状图 查询每个售价区间内汽车的销量 12345678910111213141516171819GET /cars/transactions/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot;:&#123; &quot;price&quot;:&#123; &quot;histogram&quot;:&#123; ## 定义桶类型为 histogram &quot;field&quot;: &quot;price&quot;, ## 数值字段 &quot;interval&quot;: 20000 ## 桶大小间隔，间隔 20000 意味着我们将会得到如 [0-19999, 20000-39999, ...] 这样的区间 &#125;, &quot;aggs&quot;:&#123; &quot;revenue&quot;: &#123; ## 嵌入 revenue 桶 &quot;sum&quot;: &#123; &quot;field&quot; : &quot;price&quot; ## 对每个售价区间的文档中 price 字段的值进行求和 &#125; &#125; &#125; &#125; &#125;&#125; 返回结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&#123;··· &quot;aggregations&quot;: &#123; &quot;price&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: 0, ## 代表区间 [0 - 19999] &quot;doc_count&quot;: 3, &quot;revenue&quot;: &#123; &quot;value&quot;: 37000 &#125; &#125;, &#123; &quot;key&quot;: 20000, ## 代表区间 [20000 - 39999] &quot;doc_count&quot;: 4, &quot;revenue&quot;: &#123; &quot;value&quot;: 95000 &#125; &#125;, &#123; &quot;key&quot;: 40000, &quot;doc_count&quot;: 0, &quot;revenue&quot;: &#123; &quot;value&quot;: 0 &#125; &#125;, &#123; &quot;key&quot;: 60000, &quot;doc_count&quot;: 0, &quot;revenue&quot;: &#123; &quot;value&quot;: 0 &#125; &#125;, &#123; &quot;key&quot;: 80000, &quot;doc_count&quot;: 1, &quot;revenue&quot;: &#123; &quot;value&quot;: 80000 &#125; &#125; ] &#125; &#125;&#125; extended_stats 度量 可以获得桶的平均值、标准差、最大最小值等信息12345678910111213141516171819GET /cars/transactions/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;makes&quot;: &#123; &quot;terms&quot;: &#123; ## 定义桶类型为 terms &quot;field&quot;: &quot;make&quot;, &quot;size&quot;: 10 &#125;, &quot;aggs&quot;: &#123; &quot;stats&quot;: &#123; &quot;extended_stats&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 按时间统计 查询每月销售多少台汽车 12345678910111213GET /cars/transactions/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;sales&quot;: &#123; &quot;date_histogram&quot;: &#123; ## 定义桶类型为 date_histogram &quot;field&quot;: &quot;sold&quot;, &quot;interval&quot;: &quot;month&quot;, ## 以月份为时间间隔 &quot;format&quot;: &quot;yyyy-MM-dd&quot; &#125; &#125; &#125;&#125; 返回数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&#123;··· &quot;aggregations&quot;: &#123; &quot;sales&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key_as_string&quot;: &quot;2014-01-01&quot;, &quot;key&quot;: 1388534400000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2014-02-01&quot;, &quot;key&quot;: 1391212800000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2014-03-01&quot;, &quot;key&quot;: 1393632000000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2014-04-01&quot;, &quot;key&quot;: 1396310400000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2014-05-01&quot;, &quot;key&quot;: 1398902400000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2014-06-01&quot;, &quot;key&quot;: 1401580800000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2014-07-01&quot;, &quot;key&quot;: 1404172800000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2014-08-01&quot;, &quot;key&quot;: 1406851200000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2014-09-01&quot;, &quot;key&quot;: 1409529600000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2014-10-01&quot;, &quot;key&quot;: 1412121600000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2014-11-01&quot;, &quot;key&quot;: 1414800000000, &quot;doc_count&quot;: 2 &#125; ] &#125; &#125;&#125; 强制返回空 buckets 和指定区间 上一个例子中的结果少了一些月份，而通常，你并不想要这样。对于很多应用，你可能想直接把结果导入到图形库中，而不想做任何后期加工。123456789101112131415161718GET /cars/transactions/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;sales&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;sold&quot;, &quot;interval&quot;: &quot;month&quot;, &quot;format&quot;: &quot;yyyy-MM-dd&quot;, &quot;min_doc_count&quot; : 0, ## 强制返回空 buckets &quot;extended_bounds&quot; : &#123; ## 强制返回整年 &quot;min&quot; : &quot;2014-01-01&quot;, &quot;max&quot; : &quot;2014-12-31&quot; &#125; &#125; &#125; &#125;&#125; 百分位数度量https://www.elastic.co/guide/cn/elasticsearch/guide/current/percentiles.html 过滤桶filter 只对聚合结果过滤，不对查询结果过滤 使用 过滤桶 在 查询 范围基础上应用过滤器。当文档满足过滤桶的条件时，我们将其加入到桶内 因为 filter 桶和其他桶的操作方式一样，所以可以随意将其他桶和度量嵌入其中。所有嵌套的组件都会 “继承” 这个过滤123456789101112131415161718192021222324252627GET /cars/transactions/_search&#123; &quot;size&quot; : 0, &quot;query&quot;:&#123; &quot;match&quot;: &#123; &quot;make&quot;: &quot;ford&quot; &#125; &#125;, &quot;aggs&quot;:&#123; &quot;recent_sales&quot;: &#123; &quot;filter&quot;: &#123; ## 定义过滤桶 &quot;range&quot;: &#123; &quot;sold&quot;: &#123; &quot;from&quot;: &quot;now-1M&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;average_price&quot;:&#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; post_filter 只对搜索结果过滤，不对过滤聚合结果过滤12345678910111213141516171819GET /cars/transactions/_search&#123; &quot;size&quot; : 0, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;make&quot;: &quot;ford&quot; &#125; &#125;, &quot;post_filter&quot;: &#123; &quot;term&quot; : &#123; &quot;color&quot; : &quot;green&quot; &#125; &#125;, &quot;aggs&quot; : &#123; &quot;all_colors&quot;: &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125;&#125; 多桶排序内置排序1234567891011121314GET /cars/transactions/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;_count&quot; : &quot;asc&quot; ## 按文档数升序排序 &#125; &#125; &#125; &#125;&#125; _count：按文档数排序。对 terms 、 histogram 、 date_histogram 有效。 _term：按词项的字符串值的字母顺序排序。只在 terms 内使用。 _key：按每个桶的键值数值排序（理论上与 _term 类似）。 只在 histogram 和 date_histogram 内使用。 按度量排序12345678910111213141516171819GET /cars/transactions/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;stats.variance&quot; : &quot;asc&quot; ## 多指度量只用点式路径定位 &#125; &#125;, &quot;aggs&quot;: &#123; &quot;stats&quot;: &#123; &quot;extended_stats&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125; 近似聚合cardinality 统计去重后的数量 获得经销商销售汽车颜色的数量 1234567891011GET /cars/transactions/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125;&#125; SQL形式 12SELECT COUNT(DISTINCT color)FROM cars 嵌套桶中使用：查询每个月有多少种颜色的车售出 12345678910111213141516171819GET /cars/transactions/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;months&quot; : &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;sold&quot;, &quot;interval&quot;: &quot;month&quot; &#125;, &quot;aggs&quot;: &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125; &#125; &#125;&#125; 速度优化","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"}]},{"title":"【Elasticsearch】7.2 自动补全、纠错、拼音搜索","date":"2018-05-11T08:54:40.725Z","path":"2018/05/11/Elasticsearch/7.2自动补全、纠错、拼音搜索/","text":"more_like_this 实现相似文档搜索 根据文本搜索 1234567891011GET /ik/message/_search&#123; &quot;query&quot;: &#123; &quot;more_like_this&quot; : &#123; &quot;fields&quot; : [&quot;content&quot;], &quot;like&quot; : [&quot;一个&quot;], &quot;min_term_freq&quot;: 1, ## 一篇文档中一个词语至少出现次数，小于这个值的词将被忽略，默认是2 &quot;min_doc_freq&quot;: 1 ## 一个词语最少在多少篇文档中出现，小于这个值的词会将被忽略，默认是无限制 &#125; &#125;&#125; 根据指定文档搜索 12345678910111213141516GET /ik/message/_search&#123; &quot;query&quot;: &#123; &quot;more_like_this&quot; : &#123; &quot;fields&quot; : [&quot;content&quot;], &quot;like&quot; : [ &#123; &quot;_index&quot; : &quot;ik&quot;, &quot;_type&quot; : &quot;message&quot;, &quot;_id&quot; : &quot;QOko_GIBpX-3-f32b0qP&quot; &#125;], &quot;min_term_freq&quot;: 1, &quot;min_doc_freq&quot;: 1 &#125; &#125;&#125; 根据传入的 docNum 找出该文档里去除停用词后的高频词，然后用这些高频词生成 Queue，最后把 Queue 传进 search 方法得到最后的结果。 主要思想是认为这些高频词足以表示文档信息，然后通过搜索得到最后与此 doc 类似的结果。 参数说明 suggest 自动补全或者纠错Term suggester Term suggester正如其名，只基于 analyze 过的单个 term 去提供建议，并不会考虑多个 term 之间的关系。API调用方只需为每个token挑选options里的词，组合在一起返回给用户前端即可。 准备一个叫做blogs的索引，配置一个text字段 123456789101112PUT /blogs/&#123; &quot;mappings&quot;: &#123; &quot;tech&quot;: &#123; &quot;properties&quot;: &#123; &quot;body&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 通过bulk api写入几条文档 12345678910111213POST _bulk/?refresh=true&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;blogs&quot;, &quot;_type&quot; : &quot;tech&quot; &#125; &#125;&#123; &quot;body&quot;: &quot;Lucene is cool&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;blogs&quot;, &quot;_type&quot; : &quot;tech&quot; &#125; &#125;&#123; &quot;body&quot;: &quot;Elasticsearch builds on top of lucene&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;blogs&quot;, &quot;_type&quot; : &quot;tech&quot; &#125; &#125;&#123; &quot;body&quot;: &quot;Elasticsearch rocks&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;blogs&quot;, &quot;_type&quot; : &quot;tech&quot; &#125; &#125;&#123; &quot;body&quot;: &quot;Elastic is the company behind ELK stack&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;blogs&quot;, &quot;_type&quot; : &quot;tech&quot; &#125; &#125;&#123; &quot;body&quot;: &quot;elk rocks&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;blogs&quot;, &quot;_type&quot; : &quot;tech&quot; &#125; &#125;&#123; &quot;body&quot;: &quot;elasticsearch is rock solid&quot;&#125; 执行一次 suggester 搜索 1234567891011POST /blogs/_search&#123; &quot;suggest&quot;: &#123; &quot;my-suggestion&quot;: &#123; &quot;text&quot;: &quot;lucne rock&quot;, ## 通常是用户界面上输入的内容 &quot;term&quot;: &#123; &quot;field&quot;: &quot;body&quot; &#125; &#125; &#125;&#125; 返回结果 123456789101112131415161718192021222324252627282930··· &quot;suggest&quot;: &#123; &quot;my-suggestion&quot;: [ &#123; &quot;text&quot;: &quot;lucne&quot;, &quot;offset&quot;: 0, &quot;length&quot;: 5, &quot;options&quot;: [ &#123; &quot;text&quot;: &quot;lucene&quot;, &quot;score&quot;: 0.8, &quot;freq&quot;: 2 &#125; ] &#125;, &#123; &quot;text&quot;: &quot;rock&quot;, &quot;offset&quot;: 6, &quot;length&quot;: 4, &quot;options&quot;: [ &#123; &quot;text&quot;: &quot;rocks&quot;, &quot;score&quot;: 0.75, &quot;freq&quot;: 2 &#125; ] &#125; ] &#125;&#125; Term suggester 参数文档 Phrase suggester Phrase suggester 会直接给出和用户输入文本相似的内容，在 Term suggester 的基础上，考量多个term之间的关系，比如是否同时出现在索引的原文里，相邻程度，以及词频等等。 123456789101112131415POST /blogs/_search&#123; &quot;suggest&quot;: &#123; &quot;my-suggestion&quot;: &#123; &quot;text&quot;: &quot;lucne and elasticsear rock&quot;, &quot;phrase&quot;: &#123; &quot;field&quot;: &quot;body&quot;, &quot;highlight&quot;: &#123; &quot;pre_tag&quot;: &quot;&lt;em&gt;&quot;, &quot;post_tag&quot;: &quot;&lt;/em&gt;&quot; &#125; &#125; &#125; &#125;&#125; 返回数据 12345678910111213141516171819202122232425262728··· &quot;suggest&quot;: &#123; &quot;my-suggestion&quot;: [ &#123; &quot;text&quot;: &quot;lucne and elasticsear rock&quot;, &quot;offset&quot;: 0, &quot;length&quot;: 26, &quot;options&quot;: [ &#123; &quot;text&quot;: &quot;lucne and elasticsearch rocks&quot;, &quot;highlighted&quot;: &quot;lucne and &lt;em&gt;elasticsearch rocks&lt;/em&gt;&quot;, &quot;score&quot;: 0.12709484 &#125;, &#123; &quot;text&quot;: &quot;lucne and elasticsearch rock&quot;, &quot;highlighted&quot;: &quot;lucne and &lt;em&gt;elasticsearch&lt;/em&gt; rock&quot;, &quot;score&quot;: 0.10422645 &#125;, &#123; &quot;text&quot;: &quot;lucne and elasticsear rocks&quot;, &quot;highlighted&quot;: &quot;lucne and elasticsear &lt;em&gt;rocks&lt;/em&gt;&quot;, &quot;score&quot;: 0.10036137 &#125; ] &#125; ] &#125;&#125; 因为lucene和elasticsearch曾经在同一条原文里出现过，同时替换2个term的可信度更高，所以打分较高，排在第一位返回。 Phrase Suggester 参数文档 未解决问题 Term suggester 和 Phrase Suggester 无法进行中文补全/纠错（Completion Suggester 可以） 版本差异参数设置问题 分词器 Completion Suggester 主要针对的应用场景就是”Auto Completion”。 此场景下用户每输入一个字符的时候，就需要即时发送一次查询请求到后端查找匹配项，在用户输入速度较高的情况下对后端响应速度要求比较苛刻。因此实现上它和前面两个 Suggester 采用了不同的数据结构，索引并非通过倒排来完成，而是将 analyze 过的数据编码成 FST 和索引一起存放。对于一个 open 状态的索引，FST 会被 ES 整个装载到内存里的，进行前缀查找速度极快。但是 FST 只能用于前缀查找，这也是Completion Suggester的局限所在。 为了使用 Completion Suggester，字段的类型需要专门定义如下: 123456789101112PUT /blogs_completion/&#123; &quot;mappings&quot;: &#123; &quot;tech&quot;: &#123; &quot;properties&quot;: &#123; &quot;body&quot;: &#123; &quot;type&quot;: &quot;completion&quot; &#125; &#125; &#125; &#125;&#125; 加入数据 12345678910111213POST _bulk/?refresh=true&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;blogs_completion&quot;, &quot;_type&quot; : &quot;tech&quot; &#125; &#125;&#123; &quot;body&quot;: &quot;Lucene is cool&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;blogs_completion&quot;, &quot;_type&quot; : &quot;tech&quot; &#125; &#125;&#123; &quot;body&quot;: &quot;Elasticsearch builds on top of lucene&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;blogs_completion&quot;, &quot;_type&quot; : &quot;tech&quot; &#125; &#125;&#123; &quot;body&quot;: &quot;Elasticsearch rocks&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;blogs_completion&quot;, &quot;_type&quot; : &quot;tech&quot; &#125; &#125;&#123; &quot;body&quot;: &quot;Elastic is the company behind ELK stack&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;blogs_completion&quot;, &quot;_type&quot; : &quot;tech&quot; &#125; &#125;&#123; &quot;body&quot;: &quot;the elk stack rocks&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;blogs_completion&quot;, &quot;_type&quot; : &quot;tech&quot; &#125; &#125;&#123; &quot;body&quot;: &quot;elasticsearch is rock solid&quot;&#125; 查找 1234567891011POST blogs_completion/_search?pretty&#123; &quot;size&quot;: 0, &quot;suggest&quot;: &#123; &quot;blog-suggest&quot;: &#123; &quot;prefix&quot;: &quot;elastic i&quot;, &quot;completion&quot;: &#123; &quot;field&quot;: &quot;body&quot; &#125; &#125; &#125;&#125; 返回结果 1234567891011121314151617181920212223··· &quot;suggest&quot;: &#123; &quot;blog-suggest&quot;: [ &#123; &quot;text&quot;: &quot;elastic i&quot;, &quot;offset&quot;: 0, &quot;length&quot;: 9, &quot;options&quot;: [ &#123; &quot;text&quot;: &quot;Elastic is the company behind ELK stack&quot;, &quot;_index&quot;: &quot;blogs_completion&quot;, &quot;_type&quot;: &quot;tech&quot;, &quot;_id&quot;: &quot;v-qtAGMBpX-3-f32xG7c&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;body&quot;: &quot;Elastic is the company behind ELK stack&quot; &#125; &#125; ] &#125; ] &#125;&#125; Completion Suggester 参数文档 拼音搜索 github: elasticsearch-analysis-pinyin 安装 elasticsearch-analysis-pinyin 分词器 1./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-pinyin/releases/download/v6.2.3/elasticsearch-analysis-pinyin-6.2.3.zip 创建空索引，创建分词器 1234567891011121314151617181920212223242526PUT /medcl/&#123; &quot;index&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;pinyin_analyzer&quot; : &#123; &quot;tokenizer&quot; : &quot;my_pinyin&quot; &#125; &#125;, &quot;tokenizer&quot; : &#123; &quot;my_pinyin&quot; : &#123; &quot;type&quot; : &quot;pinyin&quot;, &quot;keep_first_letter&quot;:true, &quot;keep_separate_first_letter&quot; : true, &quot;keep_none_chinese&quot; : true, &quot;trim_whitespace&quot; : true, &quot;keep_none_chinese_in_first_letter&quot; : true, &quot;keep_full_pinyin&quot; : true, &quot;keep_original&quot; : true, &quot;limit_first_letter_length&quot; : 16, &quot;lowercase&quot; : true &#125; &#125; &#125; &#125;&#125; 定义 mapping 1234567891011121314151617POST /medcl/folks/_mapping &#123; &quot;folks&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot; : &quot;ik_max_word&quot;, &quot;fields&quot;: &#123; &quot;pinyin&quot;: &#123; &quot;type&quot;: &quot;completion&quot;, ## 如果仅用于查询则设为 text &quot;analyzer&quot;: &quot;pinyin_analyzer&quot; &#125; &#125; &#125; &#125; &#125;&#125; 插入数据 1234567891011POST _bulk/?refresh=true&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;medcl&quot;, &quot;_type&quot; : &quot;folks&quot; &#125; &#125;&#123; &quot;name&quot;: &quot;刘德华&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;medcl&quot;, &quot;_type&quot; : &quot;folks&quot; &#125; &#125;&#123; &quot;name&quot;: &quot;刘力菲&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;medcl&quot;, &quot;_type&quot; : &quot;folks&quot; &#125; &#125;&#123; &quot;name&quot;: &quot;梁婉琳&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;medcl&quot;, &quot;_type&quot; : &quot;folks&quot; &#125; &#125;&#123; &quot;name&quot;: &quot;刘倩倩&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;medcl&quot;, &quot;_type&quot; : &quot;folks&quot; &#125; &#125;&#123; &quot;name&quot;: &quot;刘嘉怡&quot;&#125; 自动补全1234567891011POST medcl/_search?pretty&#123; &quot;size&quot;: 0, &quot;suggest&quot;: &#123; &quot;blog-suggest&quot;: &#123; &quot;prefix&quot;: &quot;刘dh&quot;, &quot;completion&quot;: &#123; &quot;field&quot;: &quot;name.pinyin&quot; &#125; &#125; &#125;&#125; 模糊查询12345678GET /medcl/folks/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;name.pinyin&quot;: &quot;刘dh&quot; &#125; &#125;&#125; 参数说明 keep_first_letter: 存储首字母 eg: 刘德华 &gt; ldh default: true keep_separate_first_letter: 将首字母分开存储 eg: 刘德华 &gt; l,d,h default: false limit_first_letter_length: 限制首字母最大长度 default: 16 keep_full_pinyin: 分开存储每个字的全拼 eg: 刘德华 &gt; [liu,de,hua] default: true keep_joined_full_pinyin：连接每个字的全拼进行存储 eg: 刘德华 &gt; [liudehua] default: false keep_none_chinese：保留非中文字或数字 default: true keep_none_chinese_together：将非中文字存储在一起 true: eg: DJ音乐家 -&gt; DJ,yin,yue,jia false: eg: DJ音乐家 -&gt; D,J,yin,yue,jia 注意: 需要先设置 keep_none_chinese 为true default: true, keep_none_chinese_in_first_letter：将非中文字和首字母存储在一起 eg: 刘德华AT2016 -&gt; ldhat2016 default: true keep_none_chinese_in_joined_full_pinyin: 讲非中文字和全拼音存储在一起 eg: 刘德华2016 -&gt; liudehua2016 default: false none_chinese_pinyin_tokenize: 若非中文字是拼音则将它们拆分 eg: liudehuaalibaba13zhuanghan -&gt; liu,de,hua,a,li,ba,ba,13,zhuang,han default: true 注意: 需要先将keep_none_chinese 和 keep_none_chinese_together 设置为 true keep_original: 保存源字符串 default: false lowercase：将非中文字转为小写 default: true trim_whitespace: 删除空格 default: true remove_duplicated_term: 删除重复项 eg: de的 &gt; de default: false 注意: 位置相关查询可能受影响 ignore_pinyin_offset: 在6.0之后，偏移被严格限制，不允许重叠的标记，使用这个参数，重叠的标记将被忽略偏移所允许，所有与位置相关的查询或高亮将会变得不正确，你应该使用多个字段并指定不同的设置以达到不同的查询目的。 default: true","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"}]},{"title":"【Elasticsearch】7.1 Java High Level REST Client 客户端","date":"2018-05-11T08:54:40.721Z","path":"2018/05/11/Elasticsearch/7.1JavaHighLevelRESTClient客户端/","text":"介绍 使用最新的 Java High Level REST Client 客户端方式 官网文档 官网即将在未来版本取消 Java API 接口，所以必须使用 REST API 弃用说明 JAVA API：使用的 netty 协议 TransportClient 端口9300 性能好 上手麻烦，需熟悉 API 和 ES DSL，适合大量频繁数据查询。将在7.0 开始废弃，在8.0版本移除。 JAVA REST Client：使用 http 协议 端口9200 上手简单，懂ES DSL查询即可。 配置 Maven 加入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;6.2.3&lt;/version&gt;&lt;/dependency&gt; DEMOhttps://github.com/ligohan/elasticsearch-high-level-REST-client 资料 官方API文档 demo 项目","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"}]},{"title":"【Elasticsearch】7.0 全文检索","date":"2018-05-11T08:54:40.716Z","path":"2018/05/11/Elasticsearch/7.0全文检索/","text":"关于DSL es中的查询请求有两种方式: 一种是简易版的查询，像传递URL参数一样去传递查询语句 例如 /library/books/_search?q123456 - 一种是使用JSON完整的请求体，叫做**结构化查询（DSL）**。 - 由于DSL查询更为直观也更为简易，所以大都使用这种方式。 - DSL查询是POST过去一个json，由于post的请求是json格式的，所以存在很多灵活性，也有很多形式。## DSL语句校验 GET /cars/transactions/_validate/query?explain{ “query”: { “match”: { “make”: “honda” } }}123## match_all 查询所有- 常用用法 GET /cars/transactions/_search{ “query”: { “match_all”: {} }}1- 指定查询字段 GET /cars/transactions/_search{ “query”: { “match_all”: {} }, “_source”: [ “product_name”, “price” ]}12## sort 排序 GET /cars/transactions/_search{ “query”: { “match_all”: {} }, “sort”: [ { “price”: “desc” ## 降序 } ]}12345678 &gt; - 排序最好别用到字符串字段上。因为字符串字段会进行分词，Elasticsearch 默认是拿分词后的某个词去进行排序，排序结果往往跟我们想象的不一样。&gt; - 解决这个办法是在设置 mapping 的时候，多个这个字段设置一个 fields raw，让这个不进行分词，然后查询排序的时候使用这个 raw&gt; - 具体看这里：https://www.elastic.co/guide/cn/elasticsearch/guide/current/multi-fields.html## 分页查询 GET /cars/transactions/_search{ “query”: { “match_all”: {} }, “from”: 0, ## 从第几个开始查，最开始是 0 “size”: 1 ## 要查几个结果}123## range- 用于查询数值、时间 GET /cars/transactions/_search{ “query”: { “range”: { “price”: { “gte”: 30000.00 ## 大于或等于30000 } } }}12345678- gte：大于或等于- gt：大于- lte：小于或等于- lt：小于## term- term是代表完全匹配，即**不进行分词器分析**，文档中必须包含整个搜索的词汇 GET /cars/transactions/_search{ “query”: { “term”: { “color”: “red” ## 查询的field和关键字 } }}12- **使用term要确定的是这个字段是否“被分析”(analyzed)，默认的字符串是被分析的**，例如：- 设置mapping为： PUT my_index { &quot;mappings&quot;: { &quot;my_type&quot;: { &quot;properties&quot;: { &quot;full_text&quot;: { &quot;type&quot;: &quot;text&quot; }, &quot;exact_value&quot;: { &quot;type&quot;: &quot;keyword&quot; } } } } } PUT my_index/my_type/1 { &quot;full_text&quot;: &quot;Quick Foxes!&quot;, &quot;exact_value&quot;: &quot;Quick Foxes!&quot; } 123- full_text: 指定类型为text，会被standard analyzer分词为如下terms [quick,foxes],存入倒排索引- exact_value: 指定类型为keyword，不会被分词，只有[Quick Foxes!]这一个term会被存入倒排索引- 当有以下请求： GET my_index/my_type/_search { &quot;query&quot;: { &quot;term&quot;: { &quot;exact_value&quot;: &quot;Quick Foxes!&quot; } } } // 可以请求出数据，因为exact_value字段没有被分析，可以完全匹配 GET my_index/my_type/_search { &quot;query&quot;: { &quot;term&quot;: { &quot;full_text&quot;: &quot;Quick Foxes!&quot; } } } // 请求不出数据，因为full_text分词后的结果中没有[Quick Foxes!]这个分词 12- 因此需要使用term的话，在自己设置 mapping 的时候有些不分词的字段最好设置不分词- Elasticsearch 5.X 之后给 text 类型的分词字段，又默认新增了一个子字段 keyword，这个字段的类型就是 keyword，是不分词的，默认保留 256 个字符。假设 product_name 是分词字段，那有一个 product_name.keyword 是不分词的字段，也可以用这个子字段来做完全匹配查询 PUT my_index { &quot;mappings&quot;: { &quot;my_type&quot;: { &quot;properties&quot;: { &quot;full_text&quot;: { &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: { &quot;keyword&quot;: { &quot;type&quot;: &quot;keyword&quot; } } }, &quot;exact_value&quot;: { &quot;type&quot;: &quot;keyword&quot; } } } } } 1234## match- 查询的字段内容是进行分词处理的，只要分词的单词结果中，在数据中有满足任意的分词结果都会被查询出来 GET /cars/transactions/_search{ “query”: { “match”: { “make” : { “query” : “bmw” } } }}1234567- 上面的查询匹配就会进行分词，比如&quot;宝马多少马力&quot;会被分词为&quot;宝马 多少 马力&quot;，所有有关&quot;宝马 多少 马力&quot;，那么所有包含这三个词中的一个或多个的文档就会被搜索出来- 根据lucene的评分机制(TF/IDF)来进行评分- operator： - or：默认，任意一个满足分词就可以，如上方例子 - and：必须满足分词结果中所有的词，如下方例子 GET /cars/transactions/_search{ “query”: { “match”: { “make” : { “query” : “bmw honda”, “operator”: “or” } } }}123- minimum_should_match - 必须满足分词结果中百分比的词，比如搜索词被分成这样子：java 程序员 书 推荐，这里就有 4 个词，假如要求 50% 命中其中两个词就返回，我们可以这样 GET /cars/transactions/_search{ “query”: { “match”: { “make”: { “query”: “bmw honda”, “minimum_should_match”: “50%” } } }}123## multi_match - 如果我们希望两个字段进行匹配，其中一个字段有这个文档就满足的话，使用multi_match GET /cars/transactions/_search{ “query”: { “multi_match”: { “query” : “red”, “type”: “best_fields”, “fields” : [“make”, “color”] } }}1234567- type： - best_fields：完全匹配的文档占的评分比较高 - most_fields：越多字段匹配的文档评分越高 - cross_fields：词条的分词词汇是分配到不同字段中的## 模糊查询- 在 match 查询使用 GET my_index/my_type/_search{ “query”: { “match”: { “full_text”:{ “query”: “Quic”, “fuzziness”: “AUTO” } } }}1- 在 multi_match 使用 GET /my_index/my_type/_search{ “query”: { “multi_match”: { “fields”: [ “full_text” ], “query”: “Quic”, “fuzziness”: “AUTO” } }}123## match_phrase 不分词查询- 对查询词不进行分词，必须完全匹配查询词才可以作为结果显示 GET my_index/my_type/_search{ “query”: { “match_phrase”: { “exact_value” : { “query” : “Quick Foxes!” } } }}123456#### slop参数：- 告诉match_phrase查询词条能够相隔多远时仍然将文档视为匹配- 相隔多远的意思是，你需要移动一个词条多少次来让查询和文档匹配- 为了让查询quick fox能够匹配含有quick brown fox的文档，我们需要slop的值为1： Pos 1 Pos 2 Pos 3 ----------------------------------------------- Doc: quick brown fox ----------------------------------------------- Query: quick fox Slop 1: quick ↳ fox 1- 为了让fox quick查询能够匹配我们的文档，需要slop的值为3： Pos 1 Pos 2 Pos 3 ----------------------------------------------- Doc: quick brown fox ----------------------------------------------- Query: fox quick Slop 1: fox|quick ↵ Slop 2: quick ↳ fox Slop 3: quick ↳ fox 1 GET my_index/my_type/_search { &quot;query&quot;: { &quot;match_phrase&quot;: { &quot;full_text&quot; : { &quot;query&quot; : &quot;foxes quick&quot;, &quot;slop&quot; : 3 } } } } 12345678- 尽管在使用了slop的短语匹配中，所有的单词都需要出现，但是单词的出现顺序可以不同。如果slop的值足够大，那么单词的顺序可以是任意的## bool 联合查询- must: 文档必须完全匹配条件- should: should下面会带一个以上的条件，至少满足一个条件，这个文档就符合should - 如果没有 must 条件，那么 should 中必须至少匹配一个。我们也还可以通过 minimum_should_match 来限制它匹配更多个。- must_not: 文档必须不匹配条件 GET /cars/transactions/_search{ “query”: { “bool”: { “must”: { “term”: { “color”: “red” } }, “must_not”: { “term”: { “make”: “bmw” } }, “filter”: { “range”: { “price”: { “gt”: 10000 } } } } }}` 参考资料 Elasticsearch Suggester详解","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"}]},{"title":"【Elasticsearch】6.IK-analyzer 教程","date":"2018-05-11T08:54:40.711Z","path":"2018/05/11/Elasticsearch/6.IK-analyzer教程/","text":"介绍 IKAnalyzer是一个开源的，基于java语言开发的轻量级的中文分词工具包。 IKAnalyzer3.0特性: 采用了特有的“正向迭代最细粒度切分算法“，支持细粒度和最大词长两种切分模式；具有83万字/秒（1600KB/S）的高速处理能力。 采用了多子处理器分析模式，支持：英文字母、数字、中文词汇等分词处理，兼容韩文、日文字符 优化的词典存储，更小的内存占用。支持用户词典扩展定义 针对Lucene全文检索优化的查询分析器IKQueryParser(作者吐血推荐)；引入简单搜索表达式，采用歧义分析算法优化查询关键字的搜索排列组合，能极大的提高Lucene检索的命中率。 安装解压包方式 官网下载地址 进入 Elasticsearch 插件目录：/usr/workspace/elasticsearch-6.2.3/plugins```1- 上传压缩包并解压：```unzip elasticsearch-analysis-ik-6.2.3.zip 重启Elasticsearch 联网方式 进入节点目录，输入命令： 1bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.2.3/elasticsearch-analysis-ik-6.2.3.zip 重启Elasticsearch 使用 创建一个新索引 1PUT /ik 添加一个Mapping，并设定其使用【IK】分词器，必须在往索引中添加数据前完成 Mapping，就是对索引库中索引的字段名称及其数据类型进行定义，类似于mysql中的表结构信息。不过es的mapping比数据库灵活很多，它可以动态识别字段。一般不需要指定mapping都可以，因为es会自动根据数据格式识别它的类型，如果你需要对某些字段添加特殊属性（如：定义使用其它分词器、是否分词、是否存储等），就必须手动添加mapping。 12345678910POST /ik/message/_mapping&#123; &quot;properties&quot;: &#123; &quot;fieldname&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125;&#125; ik_max_word: 会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合； ik_smart: 会做最粗粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,国歌”。 添加一些文档 1234POST /ik/message/1 &#123; &quot;content&quot;:&quot;这是一条IK Analyzer测试文档&quot;&#125; 带高亮查询 1234567891011curl -XPOST http://localhost:9200/indexname/typename/_search -H &apos;Content-Type:application/json&apos; -d&apos;&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;content&quot; : &quot;这是&quot; &#125;&#125;, &quot;highlight&quot; : &#123; &quot;pre_tags&quot; : [&quot;&lt;tag1&gt;&quot;, &quot;&lt;tag2&gt;&quot;], &quot;post_tags&quot; : [&quot;&lt;/tag1&gt;&quot;, &quot;&lt;/tag2&gt;&quot;], &quot;fields&quot; : &#123; &quot;content&quot; : &#123;&#125; &#125; &#125;&#125; 返回内容123456789101112131415161718192021222324252627282930&#123; &quot;took&quot;: 17, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.2876821, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;indexname&quot;, &quot;_type&quot;: &quot;typename&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.2876821, &quot;_source&quot;: &#123; &quot;content&quot;: &quot;这是一条IK Analyzer测试文档&quot;, &#125;, &quot;highlight&quot;: &#123; &quot;content&quot;: [ &quot;&lt;tag1&gt;这是&lt;/tag1&gt;一条IK Analyzer测试文档&quot; ] &#125; &#125; ] &#125;&#125; 配置文件 IK配置文件目录位于{conf}/analysis-ik/config 或 {plugins}/elasticsearch-analysis-ik-6.2.3/config，其中： IKAnalyzer.cfg.xml：用来配置自定义词库 main.dic：ik原生内置的中文词库，总共有27万多条，只要是这些单词，都会被分在一起 quantifier.dic：放了一些单位相关的词 suffix.dic：放了一些后缀 surname.dic：中国的姓氏 stopword.dic：英文停用词 字典配置 IK配置文件位于{conf}/analysis-ik/config/IKAnalyzer.cfg.xml 或 {plugins}/elasticsearch-analysis-ik-6.2.3/config/IKAnalyzer.cfg.xml 内容如下 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE properties SYSTEM &quot;http://java.sun.com/dtd/properties.dtd&quot;&gt;&lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!--用户可以在这里配置自己的扩展字典 --&gt; &lt;entry key=&quot;ext_dict&quot;&gt;custom/mydict.dic;custom/single_word_low_freq.dic&lt;/entry&gt; &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt; &lt;entry key=&quot;ext_stopwords&quot;&gt;custom/ext_stopword.dic&lt;/entry&gt; &lt;!--用户可以在这里配置远程扩展字典 --&gt; &lt;entry key=&quot;remote_ext_dict&quot;&gt;location&lt;/entry&gt; &lt;!--用户可以在这里配置远程扩展停用词字典--&gt; &lt;entry key=&quot;remote_ext_stopwords&quot;&gt;http://xxx.com/xxx.dic&lt;/entry&gt;&lt;/properties&gt; 停用词 停用词是指在信息检索中，为节省存储空间和提高搜索效率，在处理自然语言数据（或文本）之前或之后会自动过滤掉某些字或词 通常意义上，停用词大致分为两类。 一类是人类语言中包含的功能词，这些功能词极其普遍，与其他词相比，功能词没有什么实际含义，比如’the’、’is’、’at’、’which’、’on’等。 另一类词包括词汇词，比如’want’等，这些词应用十分广泛，但是对这样的词搜索引擎无法保证能够给出真正相关的搜索结果，难以帮助缩小搜索范围，同时还会降低搜索的效率，所以通常会把这些词从问题中移去，从而提高搜索性能。 热更新 IK 分词介绍 通过修改上述配置文件的以下两个标签可实现热更新 IK 分词 1234&lt;!--用户可以在这里配置远程扩展字典 --&gt;&lt;entry key=&quot;remote_ext_dict&quot;&gt;location&lt;/entry&gt;&lt;!--用户可以在这里配置远程扩展停止词字典--&gt;&lt;entry key=&quot;remote_ext_stopwords&quot;&gt;location&lt;/entry&gt; 其中 location 是指一个 url，比如 http://yoursite.com/getCustomDict，该请求只需满足以下两点即可完成分词热更新。 该 http 请求需要返回两个头部(header)，一个是 Last-Modified，一个是 ETag，这两者都是字符串类型，只要有一个发生变化，该插件就会去抓取新的分词进而更新词库。 该 http 请求返回的内容格式是一行一个分词，换行符用 \\n 即可。 满足上面两点要求就可以实现热更新分词了，不需要重启 ES 实例。 可以将需自动更新的热词放在一个 UTF-8 编码的 .txt 文件里，放在 nginx 或其他简易 http server 下，当 .txt 文件修改时，http server 会在客户端请求该文件时自动返回相应的 Last-Modified 和 ETag。可以另外做一个工具来从业务系统提取相关词汇，并更新这个 .txt 文件。 使用 IK 分词测试 12345GET _analyze&#123; &quot;text&quot;: &quot;抖音&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;&#125; 返回数据 123456789101112131415161718&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;抖&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 1, &quot;type&quot;: &quot;CN_CHAR&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;音&quot;, &quot;start_offset&quot;: 1, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;CN_CHAR&quot;, &quot;position&quot;: 1 &#125; ]&#125; 可见 IK 的主词典中没有“抖音”这个词，所以被拆分了 修改 IK 的配置文件 IKAnalyzer.cfg.xml 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE properties SYSTEM &quot;http://java.sun.com/dtd/properties.dtd&quot;&gt; &lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!--用户可以在这里配置自己的扩展字典 --&gt; &lt;entry key=&quot;ext_dict&quot;&gt;custom/mydict.dic;custom/single_word_low_freq.dic&lt;/entry&gt; &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt; &lt;entry key=&quot;ext_stopwords&quot;&gt;custom/ext_stopword.dic&lt;/entry&gt; &lt;!--用户可以在这里配置远程扩展字典 --&gt; &lt;entry key=&quot;remote_ext_dict&quot;&gt;http://192.168.75.128/MyIKCustomWords.php&lt;/entry&gt; &lt;!--用户可以在这里配置远程扩展停止词字典--&gt; &lt;!-- &lt;entry key=&quot;remote_ext_stopwords&quot;&gt;words_location&lt;/entry&gt; --&gt;&lt;/properties&gt; PHP方式 CentOS7.0 PHP环境搭建 在 PHP 应用目录加入MyIKCustomWords.php 12345678&lt;?phpecho &apos;抖音&apos;;header(&apos;Last-Modified: &apos;.gmdate(&apos;D, d M Y H:i:s&apos;, time()).&apos; GMT&apos;, true, 200);header(&apos;ETag: &quot;5816f349-19&quot;&apos;);echo $s; ?&gt; 重新启动Elasticsearch 可见启动过程有以下输出 12345678[2018-04-13T13:42:45,349][INFO ][o.w.a.d.Monitor ] 重新加载词典...[2018-04-13T13:42:45,349][INFO ][o.w.a.d.Monitor ] try load config from /usr/workspace/elasticsearch-6.2.3/master/config/analysis-ik/IKAnalyzer.cfg.xml[2018-04-13T13:42:45,620][WARN ][o.w.a.d.Monitor ] [Ext Loading] file not found: /usr/workspace/elasticsearch-6.2.3/master/config/analysis-ik/custom/mydict.dic[2018-04-13T13:42:45,621][WARN ][o.w.a.d.Monitor ] [Ext Loading] file not found: /usr/workspace/elasticsearch-6.2.3/master/config/analysis-ik/custom/single_word_low_freq.dic[2018-04-13T13:42:45,621][INFO ][o.w.a.d.Monitor ] [Dict Loading] http://192.168.75.128/MyIKCustomWords.php[2018-04-13T13:42:45,677][INFO ][o.w.a.d.Monitor ] 抖音[2018-04-13T13:42:45,678][WARN ][o.w.a.d.Monitor ] [Ext Loading] file not found: /usr/workspace/elasticsearch-6.2.3/master/config/analysis-ik/custom/ext_stopword.dic[2018-04-13T13:42:45,678][INFO ][o.w.a.d.Monitor ] 重新加载词典完毕... 再次进行分词 返回数据 1234567891011&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;抖音&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 0 &#125; ]&#125; 异常 配置远程扩展词典后启动 Elasticsearch 出现异常 access denied (java.net.SocketPermission 192.168.75.128:80 connect,resolve)```1234567&gt; 无法解析和连接到127.0.0.1的2005端口上，原因是在PerfectTime中设置了安全管理器＜System.setSecurityManager(new RMISecurityManager());＞，可是又没有设置访问的策略- 解决方法 - 打开文件 ../jdk*/jre/lib/security/java.policy - 修改安全策略文件，在grant &#123;&#125; 大括号中加上 ```permission java.net.SocketPermission &quot;localhost:2005&quot;,&quot;connect,resolve&quot;; MYSQL方式https://github.com/judasn/Elasticsearch-Tutorial-zh-CN/tree/master/elasticsearch-analysis-ik-5.2.0","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"},{"name":"IK-analyzer","slug":"IK-analyzer","permalink":"http://blog.shaib.cn/tags/IK-analyzer/"}]},{"title":"【Elasticsearch】5.批量操作","date":"2018-05-11T08:54:40.706Z","path":"2018/05/11/Elasticsearch/5.批量操作/","text":"介绍 批量操作好处 可以减少大量的网络开销和连接资源 bulk API 允许在单个步骤中进行多次 create 、 index 、 update 或 delete 请求 请求体格式12345&#123; action: &#123; metadata &#125;&#125;\\n ## 指定 哪一个文档 做 什么操作&#123; request body &#125;\\n&#123; action: &#123; metadata &#125;&#125;\\n&#123; request body &#125;\\n... action 必须是以下选项之一: create：如果文档不存在，那么就创建它 index：创建一个新文档或者替换一个现有的文档 update：部分更新一个文档 delete：删除一个文档 mget 批量查询 不同 index 123456789101112131415GET /_mget&#123; &quot;docs&quot;: [ &#123; &quot;_index&quot;: &quot;cars&quot;, &quot;_type&quot;: &quot;transactions&quot;, &quot;_id&quot;: &quot;Am8kzWIBWwL7t5InLKFv&quot; &#125;, &#123; &quot;_index&quot;: &quot;blogs&quot;, &quot;_type&quot;: &quot;tech&quot;, &quot;_id&quot;: &quot;WuqhAGMBpX-3-f32mmQT&quot; &#125; ]&#125; 相同 index，相同 type，不同 ID 1234GET /blogs/tech/_mget&#123; &quot;ids&quot;:[&quot;WuqhAGMBpX-3-f32mmQT&quot;, &quot;XOqhAGMBpX-3-f32mmQT&quot;]&#125; 批量增删改新增 不同 index 12345POST _bulk&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;cars&quot;, &quot;_type&quot; : &quot;transactions&quot; &#125; &#125;&#123; &quot;price&quot; : 10000, &quot;color&quot; : &quot;red&quot;, &quot;make&quot; : &quot;honda&quot;, &quot;sold&quot; : &quot;2014-10-28&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;blogs&quot;, &quot;_type&quot; : &quot;tech&quot; &#125; &#125;&#123; &quot;body&quot;: &quot;Elasticsearch builds on top of lucene&quot;&#125; 相同 index，相同 type 12345POST /cars/transactions/_bulk&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 10000, &quot;color&quot; : &quot;red&quot;, &quot;make&quot; : &quot;honda&quot;, &quot;sold&quot; : &quot;2014-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 20000, &quot;color&quot; : &quot;red&quot;, &quot;make&quot; : &quot;honda&quot;, &quot;sold&quot; : &quot;2014-11-05&quot; &#125; 删除12POST /_bulk&#123;&quot;delete&quot;: &#123;&quot;_index&quot;: &quot;cars&quot;,&quot;_type&quot;: &quot;transactions&quot;,&quot;_id&quot;: &quot;Am8kzWIBWwL7t5InLKFv&quot;&#125;&#125; 请注意 delete 动作不能有请求体，它后面跟着的是另外一个操作 修改123POST /_bulk&#123;&quot;update&quot;:&#123;&quot;_index&quot;: &quot;cars&quot;,&quot;_type&quot;: &quot;transactions&quot;,&quot;_id&quot;: &quot;Am8kzWIBWwL7t5InLKFv&quot;&#125;&#125;&#123; &quot;price&quot; : 25000, &quot;color&quot; : &quot;red&quot;, &quot;make&quot; : &quot;honda&quot;, &quot;sold&quot; : &quot;2014-11-05&quot; &#125; 可以是多个 JSON 组合起来，按 JSON 顺序执行 每个 json 内部不能换行，多个 json 之间必须换行 顺序执行过程中，前面的操作失败，不会影响后续的操作 kibana Dev Tools 可能会提示语法有错误，但是不影响操作","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"}]},{"title":"【Elasticsearch】4.索引管理、文档操作","date":"2018-05-11T08:54:40.701Z","path":"2018/05/11/Elasticsearch/4.索引管理、文档操作/","text":"索引管理 可以使用 Kibana 的 Dev Tools 执行命令 可以根据光标单独执行某一条命令 常用索引指令 查看集群健康状况：GET /_cat/health?v 查询集群中有哪些索引：GET /_cat/indices?v 简单的索引操作： 添加索引（默认配置）：PUT /user_index 删除单个索引：DELETE /user_index 删除多个索引：DELETE /user1_index,user2_index 根据通配符删除多个索引：DELETE /new_* 删除所有索引：DELETE /_all 查询索引配置信息：GET /user_index/_settings 查询多个索引配置信息：GET /user1_index,user2_index/_settings 查询所有索引配置信息：GET /_all/_settings 添加索引（自定义设置）123456789PUT /user_index&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;number_of_shards&quot;: 5, ## 分片数量 &quot;number_of_replicas&quot;: 1 ## 每个分片需要备份的数量 &#125; &#125;&#125; 修改分片备份数量 1234PUT /user_index/_settings&#123; &quot;number_of_replicas&quot;: 0&#125; 当出现有场外分片的情况下，需要对分片进行重分配 分片重分配默认是开启的，但是可能因为某些原因关闭了重分配但是忘记开启了123456PUT /_cluster/settings&#123; &quot;transient&quot;:&#123; &quot;cluster.routing.allocation.enable&quot; : &quot;all&quot; &#125;&#125; 添加索引，同时设置 mapping12345678910111213141516171819PUT /user_index&#123; &quot;settings&quot;: &#123; &quot;refresh_interval&quot;: &quot;5s&quot;, ## 索引刷新频率 &quot;number_of_shards&quot;: 5, &quot;number_of_replicas&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;student&quot;: &#123; ## 类型名称 &quot;properties&quot;: &#123; &quot;student_name&quot;: &#123; ## 字段名称 &quot;type&quot;: &quot;text&quot;, ## 字段类型 &quot;analyzer&quot;: &quot;ik_max_word&quot;, ## 设置分词器 &quot;search_analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125; &#125; &#125;&#125; mapping 可以理解为数据库中的表结构，比如设置字段的类型、默认值、是否为空等参数 先添加索引再设置 mapping123456789101112PUT /user_indexPOST /user_index/student/_mapping&#123; &quot;properties&quot;: &#123; &quot;student_name&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125;&#125; 文档操作添加文档 PUT 方式添加文档（指定ID） 12345PUT /user_index/student/1&#123; &quot;student_name&quot; : &quot;jun&quot;, &quot;student_id&quot; : &quot;1&quot;&#125; POST 方式添加文档，不指向 ID 会自动生成一个 20 位的字符串 ID。 12345POST /user_index/student&#123; &quot;student_name&quot; : &quot;jun&quot;, &quot;student_id&quot; : &quot;1&quot;&#125; 查询文档 通过 ID 查询（默认返回所有元数据）：/user_index/student/3```1- 通过 ID 查询（返回指定元数据）：```GET /user_index/student/1?_source=student_name,student_id 查询指定索引的所有数据：/user_index/student/_search```1- 设置查询超时：```GET /user_index/student/_search?timeout=5s 查询多个索引：/user_index/student/_search```1- 查询多个索引、多个类型：```GET /user1_index,user2_index/student,teacher/_search 查询所有索引、多个类型：/_all/student,teacher/_search```1- 通过通配符模糊查询多个索引：```GET /student_*/_search 通过字段查询，并按学号倒序返回：/user_index/student/_search?q1- 通过字段查询（使用减号代表不等于），并价格倒序：```GET /user_index/student/_search?q=-student_name:jun&amp;sort=student_id:desc 普通分页查询： 查询所有结果：/user_index/student/_search```1- 普通分页（查询第 1 页，每页 2 条数据）：```GET /user_index/student/_search?from=0&amp;size=2 from 是指从第几条数据开始 深度分页：详见 8.深度分页参考资料 官方文档：索引管理 官方文档：数据输入和输出","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"}]},{"title":"【Elasticsearch】3.2 Kibana 安装配置和使用","date":"2018-05-11T08:54:40.696Z","path":"2018/05/11/Elasticsearch/3.2logstash安装配置和使用/","text":"介绍 Logstash 是一个开源的数据收集引擎，它具有备实时数据传输能力。它可以统一过滤来自不同源的数据，并按照开发者的制定的规范输出到目的地。 顾名思义，Logstash 收集数据对象就是日志文件。由于日志文件来源多（如：系统日志、服务器 日志等），且内容杂乱，不便于人类进行观察。因此，我们可以使用 Logstash 对日志文件进行收集和统一过滤，变成可读性高的内容，方便开发者或运维人员观察，从而有效的分析系统/项目运行的性能，做好监控和预警的准备工作等。 安装解压包方式 官网下载地址 解压 1tar -zxvf logstash-6.2.3-linux-x86_64.tar.gz 修改 config/kibana.yml文件：/usr/workspace/kibana-6.2.3/config/kibana.yml```1- 加入以下内容 #端口 server.port: 5601 #访问的IP地址 server.host: “192.168.75.128” #es的地址 elasticsearch.url: “http://:9200” #kibana在es中的索引 kibana.index: “.kibana” #X-Pack账号密码，可在elasticsearch.yml配置关闭（详见Elasticsearch安装部署和使用.md） elasticsearch.username: “elastic” elasticsearch.password: “123456” 1234### Docker 方式- 拉取镜像 docker pull docker.elastic.co/logstash/logstash:6.2.312- 启动Kibana docker run -v /usr/local/logstash/conf.d:/usr/share/logstash/pipeline/:ro -v /tmp:/tmp:ro \\-v /usr/local/logstash/logstash.yml:/usr/share/logstash/config/logstash.yml:ro –name my-logstash \\docker.elastic.co/logstash/logstash:6.2.31234## 启动 ### 测试启动- 输入命令：```bin/logstash -e &apos;input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123; codec =&gt; rubydebug &#125; &#125;&apos; 再次输入 hello world 回车，返回结果：123456&#123; &quot;@timestamp&quot; =&gt; 2018-04-12T09:39:23.912Z, &quot;host&quot; =&gt; &quot;192.168.75.128&quot;, &quot;message&quot; =&gt; &quot;hello world&quot;, &quot;@version&quot; =&gt; &quot;1&quot; &#125; 加载配置文件启动 启动命令 1bin/logstash -f config/logstash.conf 配置文件详解 使用https://github.com/judasn/Linux-Tutorial/blob/master/markdown-file/Logstash-Base.md","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"},{"name":"ELK","slug":"ELK","permalink":"http://blog.shaib.cn/tags/ELK/"},{"name":"Logstash","slug":"Logstash","permalink":"http://blog.shaib.cn/tags/Logstash/"}]},{"title":"【Elasticsearch】3.1 Kibana 安装配置和使用","date":"2018-05-11T08:54:40.692Z","path":"2018/05/11/Elasticsearch/3.1Kibana安装配置和使用/","text":"介绍 Kibana是一个开源的分析与可视化平台，设计出来用于和Elasticsearch一起使用的。你可以用kibana搜索、查看、交互存放在Elasticsearch索引里的数据，使用各种不同的图表、表格、地图等kibana能够很轻易地展示高级数据分析与可视化。 Kibana让我们理解大量数据变得很容易。它简单、基于浏览器的接口使你能快速创建和分享实时展现Elasticsearch查询变化的动态仪表盘。安装Kibana非常快，你可以在几分钟之内安装和开始探索你的Elasticsearch索引数据，不需要写任何代码，没有其他基础软件依赖。 官网地址 安装解压包方式 官网下载地址 解压 1tar -zxvf kibana-6.2.3-linux-x86_64.tar.gz 修改 config/kibana.yml 文件： 123456789101112## 端口 server.port: 5601 ## 访问的IP地址server.host: &quot;192.168.75.128&quot; ## es的地址 elasticsearch.url: &quot;http://192.168.75.128:9200&quot; ## kibana在es中的索引 kibana.index: &quot;.kibana&quot; ## X-Pack 账号密码，可在elasticsearch.yml配置关闭（详见 2.Elasticsearch安装部署和使用.md）elasticsearch.username: &quot;elastic&quot;elasticsearch.password: &quot;123456&quot; 启动 先切换至 elastic 用户，启动 Elasticsearch 切换回 root 用户 前台启动，不能关闭终端，且启动速度较慢：12- 后台启动：```/usr/workspace/kibana-6.2.3/bin/kibana -d``` - 关闭后台：```ps -ef|grep kibana 进入 http://192.168.75.128:5601 ，即可进入 Kibana 界面 Docker方式 拉取镜像 1docker pull docker.elastic.co/kibana/kibana:6.2.3 启动Kibana 123docker run -p 5601:5601 -e &quot;ELASTICSEARCH_URL=http://localhost:9200&quot; \\-v /usr/local/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml \\--name my-kibana docker.elastic.co/kibana/kibana:6.2.3 使用 Discover：从发现页可以交互地探索ES的数据。可以访问与所选索引模式相匹配的每一个索引中的每一个文档。您可以提交搜索查询、筛选搜索结果和查看文档数据。还可以看到匹配搜索查询和获取字段值统计的文档的数量。如果一个时间字段被配置为所选择的索引模式，则文档的分布随着时间的推移显示在页面顶部的直方图中。 Visualize：可视化能使你创造你的Elasticsearch指标数据的可视化。然后你可以建立仪表板显示相关的可视化。Kibana的可视化是基于Elasticsearch查询。通过一系列的Elasticsearch聚合提取和处理您的数据，您可以创建图表显示你需要知道的关于趋势，峰值和骤降。您可以从搜索保存的搜索中创建可视化或从一个新的搜索查询开始。 Dashboard：一个仪表板显示Kibana保存的一系列可视化。你可以根据需要安排和调整可视化，并保存仪表盘，可以被加载和共享。 Timelion：Timelion是一个时间序列数据的可视化，可以结合在一个单一的可视化完全独立的数据源。它是由一个简单的表达式语言驱动的，你用来检索时间序列数据，进行计算，找出复杂的问题的答案，并可视化的结果。这个功能由一系列的功能函数组成，同样的查询的结果，也可以通过Dashboard显示查看。 Timelion 基本语法 Management：管理中的应用是在你执行你的运行时配置kibana，包括初始设置和指标进行配置模式，高级设置，调整自己的行为和Kibana，各种“对象”，你可以查看保存在整个Kibana的内容如发现页，可视化和仪表板。 Dev Tools：原先的交互式控制台Sense，使用户方便的通过浏览器直接与Elasticsearch进行交互。从Kibana 5开始改名并直接内建在Kibana，就是Dev Tools选项。","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"},{"name":"Kibana","slug":"Kibana","permalink":"http://blog.shaib.cn/tags/Kibana/"},{"name":"ELK","slug":"ELK","permalink":"http://blog.shaib.cn/tags/ELK/"}]},{"title":"【Elasticsearch】3.0 X-Pack 安装和使用","date":"2018-05-11T08:54:40.688Z","path":"2018/05/11/Elasticsearch/3.0X-Pack安装和使用/","text":"前言 安装X-Pack之前需要安装Elasticsearch、logstash和Kibana（ELK环境） Kibana 安装配置和使用 logstash 安装配置和使用 介绍 X-pack监控组件使你可以通过 Kibana 轻松地监控 ElasticSearch。您可以实时查看集群的健康和性能，以及分析过去的集群、索引和节点度量。此外，您可以监视 Kibana 本身性能。当你安装 X-pack 在群集上，监控代理运行在每个节点上收集和指数指标从 Elasticsearch。安装在 X-pack 在 Kibana 上，您可以查看通过一套专门的仪表板监控数据。 Elasticsearch 上安装 X-Pack 所有节点都需要安装联网方式安装 1bin/elasticsearch-plugin install x-pack 解压包安装- [官网下载地址](https://artifacts.elastic.co/downloads/packs/x-pack/x-pack-6.2.3.zip) - 执行命令安装 1bin/elasticsearch-plugin install file:///usr/local/software/x-pack-6.2.1.zip - 安装过程需要输入两次&apos;y&apos;，允许发送电子邮件通知，启动机器学习分析引擎等 卸载 1elasticsearch-plugin remove x-pack 关闭登陆密码 在 elasticsearch.yml 中加入1xpack.security.enabled: false Kibana上安装X-Pack 联网方式安装 1bin/kibana-plugin install x-pack 解压包安装 官网下载地址 执行命令安装 1bin/kibana-plugin install file:///path/to/file/x-pack-6.2.3.zip 在 kibana.yml 中加入 12elasticsearch.username: &quot;elastic&quot; elasticsearch.password: &quot;123456&quot; 使用 Monitoring：默认Kibana是没有该选项的。其实，Monitoring是由X-Pack集成提供的。该X-pack监控组件使您可以通过Kibana轻松地监控ElasticSearch。您可以实时查看集群的健康和性能，以及分析过去的集群、索引和节点度量。此外，您可以监视Kibana本身性能。当你安装X-pack在群集上，监控代理运行在每个节点上收集和指数指标从Elasticsearch。安装在X-pack在Kibana上，您可以查看通过一套专门的仪表板监控数据。 Graph：X-Pack图的能力使你发现一个Elasticsearch索引项是如何相关联的。你可以探索索引条款之间的连接，看看哪些连接是最有意义的。从欺诈检测到推荐引擎，对各种应用中这都是有用的，例如，图的探索可以帮助你发现网站上黑客的目标的漏洞，所以你可以硬化你的网站。或者，您可以为您的电子商务客户提供基于图表的个性化推荐。X-pack提供简单，但功能强大的图形开发API，和Kibana交互式图形可视化工具。使用X-pack图有工作与开销与现有Elasticsearch指标你不需要任何额外的数据存储的特征。","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"},{"name":"Kibana","slug":"Kibana","permalink":"http://blog.shaib.cn/tags/Kibana/"},{"name":"ELK","slug":"ELK","permalink":"http://blog.shaib.cn/tags/ELK/"},{"name":"X-Pack","slug":"X-Pack","permalink":"http://blog.shaib.cn/tags/X-Pack/"}]},{"title":"【Elasticsearch】2.Elasticsearch 安装部署","date":"2018-05-11T08:54:40.683Z","path":"2018/05/11/Elasticsearch/2.Elasticsearch安装部署/","text":"准备 Elasticsearch 需要 JAVA8 环境 CentOS7.4 已自带 检测与安装方法 安装解压包方式 下载解压包 1wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.3.tar.gz 或者从官网中下载相应的压缩包，使用ftp上传至服务器进行解压缩安装 因为 Elasticsearch 高版本不建议使用 root 用户 123456## 创建elastic用户组[root@jun ~]# groupadd elastic## 创建用户elastic## useradd elastic（用户名） -g elastic（组名） -p 123456（密码）[root@jun ~]# useradd elastic -g elastic -p 123456 解压到当前目录 1tar -zxvf elasticsearch-6.2.3.tar.gz 将Elasticsearch移动到/usr/local/elasticsearch目录之中 1mv elasticsearch-6.2.3 /usr/local/elasticsearch 添加权限 1chown -R elastic /usr/local/elasticsearch/elasticsearch-6.2.3 修改配置文件：进入 /usr/local/elasticsearch/config 目录，使用 vi 编辑器 1vi elasticsearch.yml elasticsearch.yml 配置文件详解 master节点配置参考： 123456789101112131415cluster.name: elasticsearchnode.name: masternetwork.host: 0.0.0.0http.port: 9200http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;node.master: truenode.data: truexpack.security.enabled: falsebootstrap.memory_lock: true 非master节点配置参考： 1234567891011121314151617cluster.name: elasticsearchnode.name: node1network.host: 0.0.0.0http.port: 9201http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;node.master: falsenode.data: truediscovery.zen.ping.unicast.hosts: [&quot;192.168.75.128&quot;]xpack.security.enabled: falsebootstrap.memory_lock: true 使用vi编辑器，修改 /etc/sysctl.conf 文件，添加如下代码(若无将会出现下面常见问题2)： 1vm.max_map_count=262144 退出保存后执行如下命令:1sysctl -p 使用vi编辑器，修改/etc/security/limits.conf文件，在文件末尾添加如下代码(若无将会出现下面常见问题3): 123456# elastic 为登录服务器的用户名elastic soft nofile 65536elastic hard nofile 65536elastic soft nproc 4096elastic hard nproc 4096 修改退出保存之后，切记需要退出重新登录，配置才能生效 启动Elasticsearch 1./bin/elasticsearch 查看集群状态 1234curl http://127.0.0.1:9200/_cat/healthepoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent 1475871424 16:17:04 elasticsearch green 1 1 5 5 0 0 0 0 - 100.0% Docker方式 拉取镜像 1docker pull docker.elastic.co/elasticsearch/elasticsearch:6.2.3 创建容器（单机） 1docker run -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; docker.elastic.co/elasticsearch/elasticsearch:6.2.3 Docker Compose 集群 Docker Compose 安装方法及常用命令 docker-compose.yml 文件如下 1234567891011121314151617181920212223242526272829303132333435363738394041version: &apos;2.2&apos;services: elasticsearch1: image: docker.elastic.co/elasticsearch/elasticsearch:6.2.3 container_name: elasticsearch1 environment: - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot; ulimits: memlock: soft: -1 hard: -1 volumes: - esdata1:/usr/share/elasticsearch/data - /usr/local/elasticsearch-6.2.3/esconfig/master/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml # 挂载自定义配置文件 ports: - 9200:9200 networks: - esnet elasticsearch2: image: docker.elastic.co/elasticsearch/elasticsearch:6.2.3 container_name: elasticsearch2 environment: - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot; ulimits: memlock: soft: -1 hard: -1 volumes: - esdata2:/usr/share/elasticsearch/data - /usr/local/elasticsearch-6.2.3/esconfig/node1/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml # 挂载自定义配置文件 networks: - esnetvolumes: esdata1: driver: local # /var/lib/docker/volumes esdata2: driver: local # /var/lib/docker/volumesnetworks: esnet: 常见问题 can not run elasticsearch as root Elasticsearch 5.X 版本后不可用超级管理员运行Elasticsearch，切换其他账号即可 max virutal memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 使用vi编辑器，修改 /etc/sysctl.conf 文件，添加如下代码： 1vm.max_map_count=262144 退出保存后执行如下命令: 1sysctl -p max number of threads [3750] for user [xxx] is too low, increase to at least [4096] 使用vi编辑器，修改/etc/security/limits.conf文件，在文件末尾添加如下代码(若无将会出现下面常见问题3): 123456# elastic 为登录服务器的用户名elastic soft nofile 65536elastic hard nofile 65536elastic soft nproc 4096elastic hard nproc 4096 修改退出保存之后，切记需要退出重新登录，配置才能生效","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"}]},{"title":"【Elasticsearch】1.Elasticsearch 基础知识","date":"2018-05-11T08:54:40.680Z","path":"2018/05/11/Elasticsearch/1.Elasticsearch基础知识/","text":"Elasticsearch 是什么Elasticsearch 定义 官网： 一个分布式的实时文档存储，每个字段都可以被索引与搜索 一个分布式实时分析搜索引擎 能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据 百度百科：ElasticSearch 是一个基于 Lucene 的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于 RESTful web 口，通过 HTTP 使用 JSON 进行数据索引。Elasticsearch 是用 Java 开发的，并作为 Apache 许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 维基百科：企业搜索平台，目的是组织数据并使其易于获取。 Lucene 介绍 维基百科：Lucene 不是一个完整的全文检索引擎，而是一个全文检索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎。Lucene的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎。在Java开发环境里Lucene是一个成熟的免费开放源代码工具；就其本身而论，Lucene是现在并且是这几年，最受欢迎的免费Java信息检索程序库。 Elasticsearch 历史 Elasticsearch 的由来 国内外案例： GitHub 用它来搜索达20TB的海量数据，其中包括13亿文件和1300亿行的代码 SoundCloud 使用它为1.8亿用户实时提供精确的音乐搜索服务 百度 目前广泛使用 ElasticSearch 进行文本数据分析，覆盖百度内部20多个业务线，单集群最大100台机器，200个ES节点，每天导入大于30TB的数据。 术语解释 索引 index：保存相关数据的地方，实际上是指向一个或者多个物理分片的逻辑命名空间 分片 shards：一个底层的工作单元 ，它仅保存了全部数据中的一部分。一个分片是一个 Lucene 的实例，本身就是一个完整的搜索引擎。我们的文档被存储和索引到分片内，但是应用程序是直接与索引而不是与分片进行交互 主分片：索引内任意一个文档都归属于一个主分片，技术上来说最大能够存储 Integer.MAX_VALUE - 128 个文档 副本分片：只是一个主分片的拷贝。作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务 类型 type：表示一类相似的文档，由 名称 和 映射 组成 文档 document：被序列化成 JSON 并存储到 Elasticsearch 中的对象，有指定的唯一的ID image.png 文档元数据 _index：表示文档在哪存放，即索引。因共同的特性被分组到一起的文档集合 _type：文档表示的对象类别，即类型。一类相似的文档 _id：文档唯一标识，当它和 _index 以及 _type 组合就可以唯一确定 Elasticsearch 中的一个文档。创建文档时可以自己指定ID或者让 Elasticsearch 自动生成 集群 cluster 一个运行中的 Elasticsearch 实例称为一个节点，集群是由一个或者多个拥有相同 cluster.name 配置的节点组成，它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。 集群健康监控 分布式 elasticsearch 为分布式而生，而且它的设计隐藏了分布式本身的复杂性。Elasticsearch在分布式概念上做了很大程度上的透明化，Elasticsearch致力于隐藏分布式系统的复杂性。以下这些操作都是在底层自动完成的： 将你的文档分区到不同的容器或者分片中，它们可以存在于一个或多个节点中。 将分片均匀的分配到各个节点，对索引和搜索做负载均衡。 冗余每一个分片，防止硬件故障造成的数据丢失。 将集群中任意一个节点上的请求路由到相应数据所在的节点。 无论是增加节点，还是移除节点，分片都可以做到无缝的扩展和迁移。 横向扩展和纵向扩展 Elasticsearch用于构建高可用和可扩展的系统。扩展的方式可以是购买更好的服务器（纵向扩展（ vertical scale or scaling up ））或者购买更多的服务器（ 横向扩展（ horizontal scaleor scaling out ）） Elasticsearch 虽然能从更强大的硬件中获得更好的性能，但是纵向扩展有它的局限性。真正的扩展应该是横向的，它通过增加节点来均摊负载和增加可靠性。 搜索 Elasticsearch 文档中的每个字段都将被索引并且可以被查询 。不仅如此，在简单查询时，Elasticsearch 可以使用所有这些索引字段，以惊人的速度返回结果。这是你永远不会考虑用传统数据库去做的一些事情。 它可以做到： 在类似于 gender 或者 age 这样的字段上使用结构化查询，join_date 这样的字段上使用排序，就像SQL的结构化查询一样。 全文检索，找出所有匹配关键字的文档并按照相关性（relevance） 排序后返回结果。 基于 http 协议 基于 HTTP 协议，以 JSON 为数据交互格式的 RESTful API，其他所有程序语言都可以使用 RESTful API，通过 9200 端口的与 Elasticsearch 进行通信，你可以使用你喜欢的 WEB 客户端，事实上，如你所见，你甚至可以通过 curl 命令与 Elasticsearch 通信。 Elasticsearch官方提供了多种程序语言的客户端——Groovy，Javascript， .NET，PHP，Perl，Python，以及 Ruby——还有很多由社区提供的客户端和插件，所有这些可以在文档中找到。 向Elasticsearch发出的请求的组成部分与其它普通的HTTP请求是一样的： 1234567891011121314151617181920212223curl -X&lt;VERB&gt;&apos;&lt;PROTOCOL&gt;://&lt;HOST&gt;/&lt;PATH&gt;?&lt;QUERY_STRING&gt;&apos; -d&apos;&lt;BODY&gt;&apos;• VERB HTTP 方法：GET , POST , PUT , HEAD , DELETE• PROTOCOL http 或者 https 协议（只有在 Elasticsearch 前面有https代理的时候可用）• HOST Elasticsearch集群中的任何一个节点的主机名，如果是在本地的节点，那么就叫localhost• PORT Elasticsearch HTTP服务所在的端口，默认为9200• QUERY_STRING 一些可选的查询请求参数，例如 ?pretty 参数将使请求返回更加美观易读的JSON数据• BODY 一个 JSON 格式的请求主体（如果请求需要的话）## 例如：curl -XGET &apos;localhost:9200/_count?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125;&apos;## 可简写成：GET /_count&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 准实时（ near real-time ） elasticsearch 是基于 lucene 的，lucene 是可以做到实时的，就是创建索引之后，立即能查询到。 但是这样，要么是牺牲索引的效率，每次都索引之后都刷新，要么就是牺牲查询的效率每次查询之前都进行刷新。 无论哪一种，都会让你的性能下降10倍以上，所以只能采取一种折中的方案，每隔n秒自动刷新，这样你创建索引之后，最多在ns之内肯定能查到。 这就是所谓的准实时(near real-time)查询。 资料 Elasticsearch: 权威指南","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"}]},{"title":"【Elasticsearch】0.Elasticsearch 学习大纲","date":"2018-05-11T08:54:40.671Z","path":"2018/05/11/【Elasticsearch】0.Elasticsearch学习大纲/","text":"Elasticsearch学习大纲开发环境 Elasticsearch 版本：6.2.3 系统：CentOS 7.4 x64 JDK 版本：JDK 1.8.0_131 目录 1.Elasticsearch 基础知识 2.Elasticsearch 安装配置和使用 3.X-Pack 安装配置和使用 3.1Kibana安装配置和使用 3.2logstash安装配置和使用 4.索引管理、文档操作 5.批量操作 6.IK analyzer 教程（自定义分词、停用词管理） 7.全文检索 7.1Java High Level REST Client 客户端 7.2自动补全、纠错、拼音搜索 8.聚合 9.深度分页 10.多表 Join（6.x 新特性） 11.线上部署优化 12.集群监控 13.性能测试（esrally安装和使用） 资料 Elasticsearch 6.X 新类型Join深入详解 如何在elasticsearch里面使用深度分页功能 死磕 Elasticsearch 方法论 Elasticsearch6.2.2 X-Pack部署及使用详解 ElasticSearch 集群监控 Elasticsearch Java API的基本使用 YouMeek-Elasticsearch-5.2.0-教程","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.shaib.cn/tags/Elasticsearch/"}]},{"title":"【FastDFS】2.FastDFS 与 SpringBoot 集成","date":"2018-05-10T12:35:05.261Z","path":"2018/05/10/【FastDFS】2.FastDFS与SpringBoot集成/","text":"DEMOhttps://github.com/ligohan/springboot-fastdfs-demo 整合到 SpringBoot 项目流程添加 pom 依赖12345&lt;dependency&gt; &lt;groupId&gt;com.github.tobato&lt;/groupId&gt; &lt;artifactId&gt;fastdfs-client&lt;/artifactId&gt; &lt;version&gt;1.26.2&lt;/version&gt;&lt;/dependency&gt; 将 Fdfs 配置引入项目 将注解配置加在springboot的入口类中：1234567891011```@Import(FdfsClientConfig.class)@SpringBootApplication// 解决 jmx 重复注册 bean 的问题@EnableMBeanExport(registration = RegistrationPolicy.IGNORE_EXISTING)public class JingtongApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(JingtongApplication.class, args); &#125;&#125; 在spring配置文件中加入fdfs相关配置 根据项目当中使用配置文件类型（.yml和.properties选择其中一个），加入相应的配置。application.yml123456789fdfs: soTimeout: 1500 connectTimeout: 600 thumbImage: #缩略图生成参数 width: 150 height: 150 trackerList: #TrackerList参数,支持多个 - 192.168.0.201:22122 - 192.168.0.202:22122 application.properties123456fdfs.soTimeout=1500fdfs.connectTimeout=600fdfs.thumbImage.width=150fdfs.thumbImage.height=150fdfs.trackerList[0]=192.168.0.201:22122fdfs.trackerList[1]=192.168.0.202:22122 文件上传下载工具类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package com.gitnavi.springboot.springbootsimpledemo.util;import com.github.tobato.fastdfs.domain.StorePath;import com.github.tobato.fastdfs.exception.FdfsUnsupportStorePathException;import com.github.tobato.fastdfs.service.FastFileStorageClient;import com.gitnavi.springboot.springbootsimpledemo.common.FastDFSConstants;import lombok.extern.log4j.Log4j2;import org.apache.commons.io.FilenameUtils;import org.apache.commons.lang3.StringUtils;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import org.springframework.web.multipart.MultipartFile;import java.io.ByteArrayInputStream;import java.io.IOException;import java.nio.charset.Charset;/** * Description: FastDFS文件上传下载包装类 */@Component@Log4j2public class FastDFSClientWrapper &#123; @Autowired private FastFileStorageClient storageClient; /** * 上传文件 * @param file 文件对象 * @return 文件访问地址 * @throws IOException */ public String uploadFile(MultipartFile file) throws IOException &#123; StorePath storePath = storageClient.uploadFile(file.getInputStream(),file.getSize(), FilenameUtils.getExtension(file.getOriginalFilename()),null); return getResAccessUrl(storePath); &#125; /** * 将一段字符串生成一个文件上传 * @param content 文件内容 * @param fileExtension * @return */ public String uploadFile(String content, String fileExtension) &#123; byte[] buff = content.getBytes(Charset.forName(&quot;UTF-8&quot;)); ByteArrayInputStream stream = new ByteArrayInputStream(buff); StorePath storePath = storageClient.uploadFile(stream,buff.length, fileExtension,null); return getResAccessUrl(storePath); &#125; // 封装图片完整URL地址 private String getResAccessUrl(StorePath storePath) &#123; String fileUrl = FastDFSConstants.HTTP_PRODOCOL + &quot;://&quot; + FastDFSConstants.RES_HOST + &quot;/&quot; + storePath.getFullPath(); return fileUrl; &#125; /** * 删除文件 * @param fileUrl 文件访问地址 * @return */ public void deleteFile(String fileUrl) &#123; if (StringUtils.isEmpty(fileUrl)) &#123; return; &#125; try &#123; StorePath storePath = StorePath.praseFromUrl(fileUrl); storageClient.deleteFile(storePath.getGroup(), storePath.getPath()); &#125; catch (FdfsUnsupportStorePathException e) &#123; log.warn(e.getMessage()); &#125; &#125; // 除了FastDFSClientWrapper类中用到的api，客户端提供的api还有很多，可根据自身的业务需求，将其它接口也添加到工具类中即可。 // 上传文件，并添加文件元数据 //StorePath uploadFile(InputStream inputStream, long fileSize, String fileExtName, Set&lt;MateData&gt; metaDataSet); // 获取文件元数据 //Set&lt;MateData&gt; getMetadata(String groupName, String path); // 上传图片并同时生成一个缩略图 //StorePath uploadImageAndCrtThumbImage(InputStream inputStream, long fileSize, String fileExtName, Set&lt;MateData&gt; metaDataSet); // 。。。&#125; 控制类12345678910111213141516171819202122232425262728293031323334353637383940package com.gitnavi.springboot.springbootsimpledemo.controller;import com.gitnavi.springboot.springbootsimpledemo.util.FastDFSClientWrapper;import io.swagger.annotations.Api;import io.swagger.annotations.ApiImplicitParam;import io.swagger.annotations.ApiImplicitParams;import io.swagger.annotations.ApiOperation;import org.springframework.http.ResponseEntity;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.multipart.MultipartFile;import javax.annotation.Resource;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.util.HashMap;import java.util.Map;@Api(value = &quot;FastDFSController RESTful&quot;, description = &quot;上传文件 REST API&quot;)@RestController@RequestMapping(&quot;/api/fastdfs&quot;)public class FastDFSController &#123; @Resource private FastDFSClientWrapper dfsClient; @ApiOperation(value = &quot;上传文件&quot;, notes = &quot;上传文件&quot;) @ApiImplicitParams(&#123; @ApiImplicitParam(name = &quot;file&quot;, value = &quot;文件本体&quot;, paramType = &quot;body&quot;), &#125;) @RequestMapping(value = &quot;/upload&quot;, method = RequestMethod.POST) public ResponseEntity&lt;Map&lt;String, Object&gt;&gt; upload(MultipartFile file, HttpServletRequest request, HttpServletResponse response) throws Exception &#123; Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); String fileUrl = dfsClient.uploadFile(file); map.put(&quot;file_url&quot;, fileUrl); return ResponseEntity.ok(map); &#125;&#125; 测试 image.png 资料 FastDFS 与 Springboot 集成 github - tobato/FastDFS_Client","tags":[{"name":"FastDFS","slug":"FastDFS","permalink":"http://blog.shaib.cn/tags/FastDFS/"}]},{"title":"【FastDFS】1.FastDFS 安装和部署","date":"2018-05-10T06:07:36.285Z","path":"2018/05/10/【FastDFS】1.FastDFS安装和部署/","text":"是什么 FastDFS 详细介绍：https://www.oschina.net/p/fastdfs 开发环境 FastDFS 5.12 CentOS 6.7 环境准备编译环境1yum install git gcc gcc-c++ make automake autoconf libtool pcre pcre-devel zlib zlib-devel openssl-devel -y 创建目录 说明 位置 所有安装包存放目录 /fastdfs tracker 跟踪服务器数据 /fastdfs/tracker storage 存储服务器数据 /fastdfs/storage 1234567mkdir -p /fastdfsmkdir -p /fastdfs/trackermkdir -p /fastdfs/storage# 切换到安装目录准备下载安装包# 路径可自定义，注意下方命令需要对应修改cd /usr/workspace/fastdfs 安装 libfatscommon123git clone https://github.com/happyfish100/libfastcommon.git --depth 1cd libfastcommon/./make.sh &amp;&amp; ./make.sh install 安装 FastDFS123456789git clone https://github.com/happyfish100/fastdfs.git --depth 1cd fastdfs/./make.sh &amp;&amp; ./make.sh install#配置文件准备cp /etc/fdfs/tracker.conf.sample /etc/fdfs/tracker.confcp /etc/fdfs/storage.conf.sample /etc/fdfs/storage.confcp /etc/fdfs/client.conf.sample /etc/fdfs/client.conf #客户端文件，测试用cp /usr/workspace/fastdfs/fastdfs/conf/http.conf /etc/fdfs/ #供 nginx 访问使用cp /usr/workspace/fastdfs/fastdfs/conf/mime.types /etc/fdfs/ #供 nginx 访问使用 安装 fastdfs-nginx-module12git clone https://github.com/happyfish100/fastdfs-nginx-module.git --depth 1cp /usr/workspace/fastdfs/fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs 安装 nginx123456wget http://nginx.org/download/nginx-1.12.2.tar.gztar -zxvf nginx-1.12.2.tar.gzcd nginx-1.12.2/#添加 fastdfs-nginx-module 模块./configure --add-module=/usr/workspace/fastdfs/fastdfs-nginx-module/src/make &amp;&amp; make install 单机部署配置 tracker 创建目录，用于存放 store data 和 log 1mkdir -p /usr/workspace/fastdfs/tracker/data-and-log 编辑 tracker 配置文件 1234567vim /etc/fdfs/tracker.conf# 需要修改的内容如下## tracker 服务器端口（默认 22122, 一般不修改）port=22122## 存储日志和数据的根目录base_path=/usr/workspace/fastdfs/tracker/data-and-log 保存后启动 1234/etc/init.d/fdfs_trackerd start # 启动 tracker 服务chkconfig fdfs_trackerd on # 自启动 tracker 服务/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf restart # 重启 tracker 服务 查看是否有 tracker 进程 1ps aux | grep tracker 配置 storage（存储节点） 一般 storage 服务我们会单独装一台机子，但是这里为了方便我们安装在同一台。 如果 storage 单独安装的话，那上面安装的步骤都要在走一遍，只是到了编辑配置文件的时候，编辑的是 storage.conf 而已配置流程 创建目录，用于存放 store data 和 log 1mkdir -p /usr/workspace/fastdfs/storage/data-and-log 创建图片实际存放路径，可以设置多个 12mkdir -p /usr/workspace/fastdfs/storage/images-data0mkdir -p /usr/workspace/fastdfs/storage/images-data1 编辑 storage 配置文件 1234567891011121314vim /etc/fdfs/storage.conf# 需要修改的内容如下port=23000 # storage 服务端口（默认 23000, 一般不修改）base_path=/usr/workspace/fastdfs/storage/data-and-log # 数据和日志文件存储根目录## 第一个存储目录store_path0=/usr/workspace/fastdfs/storage/images-data0## 如果有多个，可以配置多行## store_path1=/usr/workspace/fastdfs/storage/images-data1## store_path2=/usr/workspace/fastdfs/storage/images-data2## tracker 服务器的 IP 和端口tracker_server=192.168.75.128:22122## http 访问文件的端口（默认 8888, 看情况修改, 和 nginx 中保持一致）http.server_port=8888 保存后启动 1234567# 启动 storage 服务，首次启动会很慢，因为它在创建预设存储文件的目录/etc/init.d/fdfs_storaged start# 自启动 storage 服务chkconfig fdfs_storaged on# 重启 storage 服务/usr/bin/fdfs_storaged /etc/fdfs/storage.conf restart 查看是否有 storage 进程 1ps aux | grep storage client 测试 创建存放 store log 目录 1mkdir -p /usr/workspace/fastdfs/fastdfs/client/data-and-log 修改 client 配置文件 1234567vim /etc/fdfs/client.conf# 需要修改的内容如下## 存储日志和数据的根目录base_path=/usr/workspace/fastdfs/fastdfs/client/data-and-log## tracker 服务器的 IP 和端口tracker_server=192.168.75.128:22122 保存后测试 12345678# 先使用 shell 上传一张测试图片 test.jpg# 运行测试fdfs_upload_file /etc/fdfs/client.conf /usr/workspace/fastdfs/test/1.jpg# 返回以下 ID 即为成功group1/M00/00/00/wKhLgFryZoOATuwbAANbszC8Hfo381.jpg# 安装完 Nginx 后即可通过访问以下地址下载http://192.168.75.128/group1/M00/00/00/wKhLgFryZoOATuwbAANbszC8Hfo381.jpg 配置 Nginx 创建保存 log 目录 1mkdir -p /usr/workspace/fastdfs/fastdfs-nginx-module/data-and-log 编辑 Nginx 模块配置文件 123456789101112131415vim /etc/fdfs/mod_fastdfs.conf# 需要修改的内容如下## tracker 服务器的 IP 和端口tracker_server=192.168.75.128:22122## log 存放目录base_path=/usr/workspace/fastdfs/fastdfs-nginx-module/data-and-log## 因为我们访问图片的地址是：http://192.168.1.114/group1/M00/00/00/wKgBclb0aqWAbVNrAAAjn7_h9gM813_big.jpg## 该地址前面是带有 /group1/M00，所以我们这里要使用 true，不然访问不到（原值是 false）url_have_group_name=true## 第一个存储目录store_path0=/usr/workspace/fastdfs/storage/images-data0## 如果有多个，可以配置多行## store_path1=/usr/workspace/fastdfs/storage/images-data1## store_path2=/usr/workspace/fastdfs/storage/images-data2 编辑 Nginx 配置文件 12345678910111213141516171819202122232425vi /usr/local/nginx/conf/nginx.conf# 改为以下内容## 注意这一行，我特别加上了使用 root 用户去执行，不然有些日记目录没有权限访问user root;worker_processes 1;events &#123;worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; ## 访问本机 server_name 192.168.75.128; ## 拦截包含 /group1/M00 请求，使用 fastdfs 这个 Nginx 模块进行转发 location /group1/M00 &#123;ngx_fastdfs_module;&#125; &#125;&#125; 启动 Nginx 停掉防火墙：iptables stop```1- 启动：```/usr/local/nginx/sbin/nginx 访问：to nginx!```，即可表示安装成功12- 检查 时候有 Nginx 进程：```ps aux | grep nginx```，正常是显示 3 个结果出来- 刷新 Nginx 配置后重启：```/usr/local/nginx/sbin/nginx -s reload 停止 Nginx：-s stop```1- 如果访问不了，或是出现其他信息看下错误立即：```vim /var/log/nginx/error.log 测试下载 用外部浏览器访问刚才已传过的 nginx 安装包, 引用返回的 IDhttp://192.168.75.128/group1/M00/00/00/wKhLgFryZoOATuwbAANbszC8Hfo381.jpg 弹出下载单机部署全部跑通，否则首先检查防火墙，再检查其他配置。 集群部署https://www.cnblogs.com/cnmenglang/p/6731209.html 参考资料 github - judasn/Linux-Tutorial/FastDFS-Install-And-Settings 官方 GitHub Wiki FastDFS 之集群部署","tags":[{"name":"FastDFS","slug":"FastDFS","permalink":"http://blog.shaib.cn/tags/FastDFS/"}]}]